[
  {
    "objectID": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#introduction",
    "href": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#introduction",
    "title": "How are machine learning models deployed?",
    "section": "Introduction",
    "text": "Introduction\nDeploying machine learning models is an essential step in applying data science to real-world problems. After training a model, the next step is to deploy it in a way that allows it to be used by others. But how do you deploy a data science model, and what are the best ways to do it?\nThere are different ways to deploy machine learning models, and the choice of deployment method depends on the specific requirements of a project. In this blog post, we will explore at a high-level the most common conceptual deployment methods for data science models and their use cases.\nThis post will not go into any platform specific details but rather create conceptual buckets to understand the different ways to deploy a model. Please comment if you believe any are missing."
  },
  {
    "objectID": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#api-deployment",
    "href": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#api-deployment",
    "title": "How are machine learning models deployed?",
    "section": "API Deployment",
    "text": "API Deployment\nOne common way to deploy machine learning models is through building an API. This involves creating an API that can receive input data, pass it through the model, and return the predicted output. This approach is suitable for models that require real-time prediction and easy integration with other systems. API deployment is also useful when working with applications that require continuous integration and deployment.\nUsing FastAPI is a great way to quickly build a working API with built-in documentation. I have used this to quickly create an API to serve predictions to other clients/services within the company. For instance, a mobile or webapp could call the API for a predicted price to display to a user. this makes it easy to serve predictions to multiple device types without creating separate model deployment code for each."
  },
  {
    "objectID": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#batch-processing",
    "href": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#batch-processing",
    "title": "How are machine learning models deployed?",
    "section": "Batch Processing",
    "text": "Batch Processing\nAnother approach to deploying machine learning models is batch processing. In this method, the model is run on a schedule, such as daily or weekly, to process large amounts of data. Batch processing is best suited for models that need to process large amounts of data or for models that don’t require real-time predictions.\nThe batch processing method is underappreciated in my opinion. The fact is that you don’t need real-time predictions to get value out of machine learning. An excellent use-case I have used in the past is for data enrichment. This can be many things: cleaning up data, filling in missing values, creating embeddings for other models to use, or creating forecasts. For instance, you may publish a monthly or quarterly forecast for users of your product. These forecasts would be batch processed and stored in a database for further use."
  },
  {
    "objectID": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#messaging-service",
    "href": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#messaging-service",
    "title": "How are machine learning models deployed?",
    "section": "Messaging Service",
    "text": "Messaging Service\nMessaging service deployment involves using a messaging service, such as RabbitMQ or Kafka, to handle asynchronous tasks. The model is deployed as a worker that listens to a queue and processes the data. This approach allows for easy scaling and real-time prediction.\nDepending on the infrastructure set up at your company, this might be an easy option for model deployment. It isn’t hard to set up a Kafka consumer to listen for requests and then publish those results by using a producer. The one caveat with this method is if the message you’re consuming from doesn’t have all the data your model needs, getting that data may make the service more complicated or slower to respond. For instance, if you want to add regional weather data you might need to use a slow database query before generating the producer message. An alternative is to pre-load the data into a caching database like Redis for a quick lookup. However, this complicates the deployment and adds another layer of maintenance and management."
  },
  {
    "objectID": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#serverless",
    "href": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#serverless",
    "title": "How are machine learning models deployed?",
    "section": "Serverless",
    "text": "Serverless\nServerless deployment involves using a serverless platform, such as AWS Lambda or Google Cloud Functions, to deploy the model. This approach allows for low-cost, easy scaling and eliminates the need for server management. This deployment method is useful for organizations that need to deploy models quickly and at a low cost.\nWhile I have not deployed a model using a serverless platform, there are many great tutorials out there that show how easy it can be. Here are a couple: one using Google Cloud Functions, and using AWS Lambda. Of course the platforms themselves have tutorials too (AWS, GCP)."
  },
  {
    "objectID": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#embedded-devices",
    "href": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#embedded-devices",
    "title": "How are machine learning models deployed?",
    "section": "Embedded Devices",
    "text": "Embedded Devices\nDeploying the model on edge devices, such as IoT devices or mobile phones, is known as embedded device deployment. This approach is suitable for models that require low latency predictions or models that need to operate in low-bandwidth environments.\nWhile I’m fascinated by edge devices like RaspberryPi, I have yet to train and deploy a model on one. RaspberryPi Machine Learning Simply Explained is a fun tutorial that turns a RaspberryPi into something… more."
  },
  {
    "objectID": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#desktop-and-web-based-applications",
    "href": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#desktop-and-web-based-applications",
    "title": "How are machine learning models deployed?",
    "section": "Desktop and Web-based Applications",
    "text": "Desktop and Web-based Applications\nDesktop application deployment involves deploying the model as a standalone application that runs on a user’s desktop or laptop. This approach is suitable for models that require a user-friendly interface or models that need to be used by a small number of users. Web-based application deployment involves deploying the model as a web application, where users can interact with the model through a browser. This approach is suitable for models that need to be accessible to a large number of users.\nWhile technically another option to deploy a machine learning model, I think this deployment method is much less common. Realistically, a webapp likely calls a backend API for the predictions rather than being embedded into the app itself, but maybe I’m wrong here."
  },
  {
    "objectID": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#hybrid-mode",
    "href": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#hybrid-mode",
    "title": "How are machine learning models deployed?",
    "section": "Hybrid Mode",
    "text": "Hybrid Mode\nLastly, machine learning deployment can employ a mix of two or more(?) methods. Hybrid deployment involves deploying the model in multiple ways, such as an API, batch processing, and messaging service. This approach allows organizations to take advantage of the benefits of different deployment methods and is suitable for models that have different use cases. This could be for a variety of reasons such as different entry-points or different levels of quality. For instance, there are dictation software with online and offline modes. The online is better but requires a sufficient online connection."
  },
  {
    "objectID": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#conclusion",
    "href": "posts/2022-06-12-how-are-machine-learning-models-deployed/index.html#conclusion",
    "title": "How are machine learning models deployed?",
    "section": "Conclusion",
    "text": "Conclusion\nThere are different ways to deploy data science models, and the choice of deployment method depends on the specific requirements of a project. The deployment method chosen must provide an optimal balance between cost, latency, and scalability. Personally, I have deployed using APIs, a messaging service, and batch processing. Have you deployed a data science model before? If so, what method did you use, and how did it work out for you? Let us know in the comments below."
  },
  {
    "objectID": "posts/2018-12-15-Taking-a-bat-to-scripts/index.html",
    "href": "posts/2018-12-15-Taking-a-bat-to-scripts/index.html",
    "title": "Taking a .bat to scripts",
    "section": "",
    "text": "Let’s suppose that we need to periodically run the same scripts (written in R or Python perhaps), and furthermore we’re using Windows. One of many possible solutions to this problem is to use a .bat file and then use a task scheduler to periodically run those scripts.\nBut why use a .bat file? If you are automating scripts, then just run the script! In Windows, a .bat file has the advantage of being flexibly executed. I can double click on it, use the cmd prompt, or I can throw it into a task scheduler and it will run. This flexibility is especially useful if you inconsistently execute the script or have not-so-tech-savvy end users.\nBut with this flexibility does come a challenge. How do .bat files pass absolute/relative paths to internal script calls? To solve this problem, I developed a quick test case to experiment. I wrote my test case in R but could have just as easily written it in Python."
  },
  {
    "objectID": "posts/2018-12-15-Taking-a-bat-to-scripts/index.html#test-case",
    "href": "posts/2018-12-15-Taking-a-bat-to-scripts/index.html#test-case",
    "title": "Taking a .bat to scripts",
    "section": "Test case",
    "text": "Test case\nMy project directory contains the .bat file and separate folders for data and the R script.\nThe directory looks like this:\n\nIn the R script, I reference a file in the data folder.\ntest_data <- readRDS(\"./data/test_data.rds\")\n\nprint(getwd())\nprint(test_data)\n\nfileConn <- file(\"result.txt\")\nwriteLines(getwd(), fileConn)\nclose(fileConn)\nThis code will use a relative path to retrieve data from a separate folder and expects the root path “./” to be the project directory.\nI’m not an expert at writing .bat files so I wrote a couple different scenarios to see what would happen. Eventually, I came up with the following code with comments. To the batch file (.bat) experts out there, please let me know if there is a better way.\n@echo off\nREM This is a test to determine the working path \nREM when running RScript in BAT\nREM %CD% gives the path the script was called from\nREM %~dp0 will give the path of the script itself\n\necho Called path:\necho %CD%\necho.\necho Script path:\necho %~dp0\necho.\n\npause\n\necho Works only if you run it from the .bat's directory:\nRscript \"%~dp0\\R\\bat_test.R\"\n\necho.\necho Works even if you run it from another directory:\ncd %~dp0\nRscript \"R\\bat_test.R\"\necho.\n\necho Press enter to close . . . \npause >nul\n\nexit\nHere were my results:\n\nAs you can see, I called the .bat file from my user profile directory. The first attempt fails but the second is a success. By using this second method, a user can flexibly execute the .bat file by double clicking, navigating from the cmd prompt, or using the task scheduler."
  },
  {
    "objectID": "posts/2021-05-30-getting-started-with-julia-in-vscode/index.html",
    "href": "posts/2021-05-30-getting-started-with-julia-in-vscode/index.html",
    "title": "Getting Started with Julia in VS Code",
    "section": "",
    "text": "This project was just first attempt creating a Julia Project using VS Code. Workflow is super important to me so I generally try to understand that first when learning a new language. I found the video by David Anthoff and decided to follow along and take notes along the way. This repository is the result. If you watch the video, he assumes you already have some familiarity with VS Code and glosses over certain keyboard shortcuts. I try my best to explicitly explain the steps below.\nThat being said, I assume you have already installed julia and the VS Code julia language extension. I also noticed that not all of the features worked for me as I followed along. I can only assume that there were some breaking changes in either the language, extension, or VS Code."
  },
  {
    "objectID": "posts/2021-05-30-getting-started-with-julia-in-vscode/index.html#creating-an-new-project",
    "href": "posts/2021-05-30-getting-started-with-julia-in-vscode/index.html#creating-an-new-project",
    "title": "Getting Started with Julia in VS Code",
    "section": "Creating an New Project",
    "text": "Creating an New Project\n\nWithin VS Code, pressing shift+cmd+p (control+shift+p on Windows) will open the command palette; after which type Julia: Start REPL to start a julia terminal\nWithin the REPL type ] to enter package mode, and activate . to activate a new environment\nType add DataFrames Query VegaLite VegaDatasets to add the desired project libraries\n\nNotice there are now a Project.toml and Manifest.toml files in your directory which describe your requirements and dependencies respectively for this project\nIf you are working in a previously created project, you can call instantiate (which will use the existing Project.toml and Manifest.toml files) instead of add DataFrames ....\n\nNow click on the Julia env: v1.x button in the status bar and select this directory as the environment. Your environment should update your directory’s name: Julia env: <your_dir_name>.\nTo get back to the Julia REPL, press backspace or ^C\n\n\nThings to Know\n\nTo executed code interactively from the file into the terminal, use control+enter. This is different than in Python where users press shift+enter.\nTo restart the terminal, you can close the existing terminal by selecting the trash can or by shift+cmd+p and typing Julia: Stop REPL before creating a new one by shift+cmd+p and typing Julia: Start REPL. I did not find an easy way to restart the session.\nDevcontainers are a great way to create an isolated environment. All you need is a .devcontainer folder with a devcontainer.json file. In this case we had the following simple code to direct the creation of a dev docker container:\n\n{\n    \"image\": \"julia\", \n    \"extensions\": [\"julialang.language-julia\"]\n}\nThis tells VS Code to build a container and install using the julia image with the julialang VS Code extension. After the docker container is built, a user can activate . and instatiate to start working in the clean environment."
  },
  {
    "objectID": "posts/2023-02-25-customizing-your-quarto-website/index.html",
    "href": "posts/2023-02-25-customizing-your-quarto-website/index.html",
    "title": "Customizing Your Quarto Website",
    "section": "",
    "text": "Whether you’re new to Quarto or recently migrated from Fastpages to Quarto, you might be wondering how to customize your new Quarto website. While Quarto provides more flexibility and control over formatting and content, it can take some time to get familiar with the system and make it your own.\nIn this blog post, I will share some tips and tricks that I’ve discovered to help you customize your Quarto website and improve its functionality and engagement."
  },
  {
    "objectID": "posts/2023-02-25-customizing-your-quarto-website/index.html#inspiration",
    "href": "posts/2023-02-25-customizing-your-quarto-website/index.html#inspiration",
    "title": "Customizing Your Quarto Website",
    "section": "Inspiration",
    "text": "Inspiration\nHere are a few Quarto blogs which have inspired me. I hope they can inspire you too!\n\nSalman Naqvi\nMike Mahoney\nBea Milz\nTom Mock\nAlbert Rapp\n\nHe also has an excellent guide for creating a Quarto blog"
  },
  {
    "objectID": "posts/2023-02-17-migrating-from-fastpages-to-quarto/index.html",
    "href": "posts/2023-02-17-migrating-from-fastpages-to-quarto/index.html",
    "title": "Migrating from Fastpages to Quarto",
    "section": "",
    "text": "Many years ago, I created my first blog using Jekyll. But since I’m not an avid web developer, if I took a long break from blogging, I’d have to re-learn how to build and customize the site. Additionally, as a data scientist I needed a way to showcase my work and Jekyll didn’t have a way to easily turn Jupyter notebooks into blog posts.\nThen while taking the online course Practical Deep Learning for Coders, I learned about Fastpages. Its main selling point was to easily create and publish Jupyter notebooks as blog posts. As many data scientists or machine learning engineers, Fastpages turned into my go-to tool for my personal site and blog. But recently, a new kid on the block has emerged - Quarto.\nIn this blog post, I’ll walk you through what I did to migrate from Fastpages to Quarto. I’ll cover the benefits of using Quarto, the steps you need to take to migrate your content, and some useful tips to help you get started."
  },
  {
    "objectID": "posts/2023-02-17-migrating-from-fastpages-to-quarto/index.html#why-migrate-from-fastpages-to-quarto",
    "href": "posts/2023-02-17-migrating-from-fastpages-to-quarto/index.html#why-migrate-from-fastpages-to-quarto",
    "title": "Migrating from Fastpages to Quarto",
    "section": "Why Migrate from Fastpages to Quarto?",
    "text": "Why Migrate from Fastpages to Quarto?\nWell, the number one reasons is that Fastpages is now deprecated and the authors recommend to migrate to Quarto. Fastpages was a great way to turn Jupyter notebooks into blog posts quickly. However, the limitations of the tool became apparent over time. Fastpages are not as customizable as one might like, and the formatting of blog posts is not very flexible. It was built on Jekyll and so, while you can add lots of functionality, you’d probably need to learn more Jekyll than most people would like and understand how that code interacts with Fastpages.\nQuarto, on the other hand, is a modern document publishing system that is designed to be flexible, powerful, and easy to use. With Quarto, you have more control over the formatting of your blog posts, and you can customize the look and feel of your blog to match your personal style. Additionally, Quarto supports multiple output formats, so you can publish your content to HTML, PDF, or other formats.\nIf you are familiar with Rmarkdown documents, Quarto is the spiritual successor to Rmarkdown with a number of excellent features. My favorite is being able to run code inline using Python, R, Observable JS, or Julia."
  },
  {
    "objectID": "posts/2023-02-17-migrating-from-fastpages-to-quarto/index.html#steps-to-migrate-from-fastpages-to-quarto",
    "href": "posts/2023-02-17-migrating-from-fastpages-to-quarto/index.html#steps-to-migrate-from-fastpages-to-quarto",
    "title": "Migrating from Fastpages to Quarto",
    "section": "Steps to Migrate from Fastpages to Quarto",
    "text": "Steps to Migrate from Fastpages to Quarto\nI followed the post by nbdev.fast.ai to get started. My instructions below differ a bit for various reasons I explain.\nInstructions:\n\nInstall Quarto\nCreate a new repo or directory to migrate your blog to\nIn this new repo, create a quarto blog and install required extensions with the following terminal commands. This will create a minimal project structure for you:\n\nquarto create-project --type website:blog .\nYou do not need to run quarto install extension quarto-ext/video since the extension is now part of the base Quarto as of 1.2. I submitted an issue for this, so it might be corrected.\n\nYour new repo will have a posts/ directory. This is where you will copy all of your notebook and markdown posts from Fastpages. You can use the code provided by the nbdev.fast.ai post but I found it just as easy to manually copy the _notebooks and _posts content to the new blog.\nNow you need to copy any images from the old blog to the new blog. This was the least manual and most annoying part form me. I used the code they provided, but it didn’t turn out quite the way I hoped. Many links were still broken after the step 6 merger.\n\ncp ../blog/images/* posts\ncp -r ../blog/images/copied_from_nb/* posts/\n\nMake your posts Quarto compatible with the following command:\n\nnbdev_migrate --path posts\n\n\n\n\n\n\nWhat does nbdev_migrate do?\n\n\n\n\n\nnbdev_migrate does the following things:\nFor notebooks - Migrates markdown front matter to raw cell front matter as described here. - nbdev v1 directives are automatically converted to Quarto directives. Note that we convert everything to Quarto directives (nbdev-specific directives are not relevant for this context) - Markdown shortcut for embedding youtube videos and callouts are automatically converted to work with Quarto.\nFor markdown and notebooks - Automatically creates link aliases so that old links will not break. Jekyll automatically generates URLs differently than Quarto, so this ensures that the Jekyll way is aliased. - Automatically corrects image paths - Makes front matter compatible with Quarto by changing field names and values where necessary\n\n\n\n\nAfter running the migration code, I found many references to images were broken. But I needed to clean up the file structure anyway. I put every post into it’s own named folder and renamed the content files to index.qmd. This is different than the Fastpages naming convention and structure. Personally, I like the Quarto setup better since the assets and next to the content and don’t require complicated path references anymore. After you get the file structure the way you like it, you’ll need to make sure the links work correctly. An easy way to check is by serving the site to your localhost by using the command quarto preview. Check each page and fix any broken links you find. Here is the example file structure from the nbdev.fast.ai post:\n\nnbs/blog\n├── index.qmd\n└── posts\n    ├── 2022-07-28-nbdev2\n    │   ├── cover.png\n    │   ├── index.qmd\n    │   ├── ...\n    └── 2022-08-25-jupyter-git\n        ├── friendly-conflict.png\n        ├── index.qmd\n        └── ...\n    ...\nAt this point the migration is basically complete. They also recommend updating the following files:\n\n./.gitignore: we suggest adding _site/ as well as dot files .*\n./about.qmd: Add some information about yourself.\n./profile.jpg: optionally change the profile picture.\n./_quarto.yml: update the social media links\n\nIf you have index files with different extensions you can edit the list generator in your home page (./index.qmd) like this:\n---\ntitle: \"Listing Example\"\nlisting:\n  contents:\n    - \"reports/*.qmd\"\n    - \"reports/*.md\"\n    - \"lab-notes/*reports.ipynb\"\nI’ve noticed that if you use the wildcard (*) for file extensions, then it causes issues when populating the lists. Specifically, in my case it counted draft posts and put them in blank cells within the list. It’s a good thing it is easy to specify different extension types as shown above.\nOnce you get the website the way you like, you can publish your site by following the Publishing Basics guide. I use GitHub Pages so I used that option with the quarto publish command for publishing.\nContinue reading my journey of migrating to Quarto in my second post, Customizing Your Quarto Website, where I talk about improvements and customizations I have used to improve the website."
  },
  {
    "objectID": "posts/2023-03-03-building-credibility/index.html",
    "href": "posts/2023-03-03-building-credibility/index.html",
    "title": "Building Credibility: The Key to Success",
    "section": "",
    "text": "Establishing credibility is crucial in achieving our goals, whether it’s getting a job, launching a successful business, or becoming an influencer. It’s the foundation that builds trust and confidence, which can make or break our success. However, establishing credibility can be a challenging task, especially when we lack relevant job experience or formal qualifications.\nWhen I got out of the military and started applying for jobs, I realized I likely wouldn’t get the job in data science I wanted. I didn’t have job experience even though I had a Masters degree. I wasn’t sure if I should go back to school and get a PhD, or get a transition job to get some experience. During this time, I thought a lot about credibility.\nIn this blog post, we will explore the concept of credibility and its three essential elements: experience, credentials, and social backing. We’ll look at how each element plays a crucial role in building credibility and how they work together to establish trust and confidence in our abilities. Whether you’re just starting your career or looking to make a pivot, this post will provide you with valuable insights and practical tips to help you build credibility and achieve your goals."
  },
  {
    "objectID": "posts/2023-03-03-building-credibility/index.html#experience",
    "href": "posts/2023-03-03-building-credibility/index.html#experience",
    "title": "Building Credibility: The Key to Success",
    "section": "Experience",
    "text": "Experience\n\n“I have a track record of success. I’ve done this for a long time and seen it all.”\n\nExperience is one of the key elements of credibility. It shows we can do the work because we have done it before. However, gaining relevant experience can be a challenging task, especially when starting in a new career field.\nWhen I was looking for a job in data science, I had no experience in the field, and this made it challenging to get my foot in the door. However, I did not let this discourage me. Instead, I started working on small projects, volunteered my skills to various organizations, and joined relevant communities. These experiences not only helped me gain new skills and knowledge but also gave me the confidence to showcase my abilities to potential employers.\nIf you’re looking to build your experience, there are many ways to do so. Consider taking on side projects, freelancing, or volunteering for organizations that align with your goals. Participating in hackathons, online courses, and industry events are also great ways to gain hands-on experience and network with professionals in your field. By building your experience, you’ll gain valuable insights into the industry and show potential employers that you have the skills and expertise they’re looking for."
  },
  {
    "objectID": "posts/2023-03-03-building-credibility/index.html#credentials",
    "href": "posts/2023-03-03-building-credibility/index.html#credentials",
    "title": "Building Credibility: The Key to Success",
    "section": "Credentials",
    "text": "Credentials\n\n“I have certificates, certifications, degrees, etc. that prove I know what I’m talking about.”\n\nCredentials play a significant role in establishing credibility as they demonstrate your knowledge and expertise in a particular field. Formal qualifications such as degrees and certifications are often relied upon by people to establish their credibility. However, having such credentials is not always necessary, and sometimes it may not be enough. It’s essential to recognize that credentials are just one part of the equation, and they must be combined with other elements of credibility to establish a strong reputation.\nIn my personal experience, I struggled with the best way to improve my credentials. Before getting a job, I had to decide whether to leverage my master’s, get additional certifications, or go back to school for a PhD. In the end, I chose to take online supplemental courses and build up my skills through personal projects. Your situation might be different, but you may encounter similar decisions on your journey towards your goals.\nMoreover, it’s worth noting that credentials are not limited to formal education alone. Training programs and even personal projects can all count as credentials. The key is to identify the credentials that are most relevant to your goals and showcase them effectively."
  },
  {
    "objectID": "posts/2023-03-03-building-credibility/index.html#social-backing",
    "href": "posts/2023-03-03-building-credibility/index.html#social-backing",
    "title": "Building Credibility: The Key to Success",
    "section": "Social Backing",
    "text": "Social Backing\n\n“People recognize me as an expert. And I am an expert because otherwise, people wouldn’t recognize me.”\n\nSocial backing is the third and final element of credibility. It refers to the recognition and support a person receives from their community, peers, and followers. Social backing has always been an essential element of credibility, however in our increasingly digital age, technology amplifies outreach and impact echos like never before. So in today’s modern world, where social media has a significant influence on people’s lives, social backing plays a crucial role in establishing credibility.\nSocial backing is community-driven and self-fulfilling to a degree. People tend to listen to those who have a significant following or are recognized as experts in their field. For instance, an influencer with a massive following on social media can quickly establish credibility and influence their followers’ opinions and decisions.\nBuilding social backing takes time and effort, but it can be a rewarding experience. Attending meetups, volunteering to give talks, writing tutorials, and posting blog articles can help in establishing social backing. Networking on LinkedIn and in-person can also help in building a community of like-minded people who recognize your expertise and support you in your endeavors.\nPersonally, I have found that building social backing is a continuous process. It requires consistent effort to stay engaged with the community and build relationships with peers and followers. I have not always done this well and still have lots to learn. In my experience, contributing to online forums and groups, participating in social media conversations, and collaborating with others on projects can help in establishing social backing."
  },
  {
    "objectID": "posts/2023-03-03-building-credibility/index.html#the-interplay-of-the-three-elements",
    "href": "posts/2023-03-03-building-credibility/index.html#the-interplay-of-the-three-elements",
    "title": "Building Credibility: The Key to Success",
    "section": "The Interplay of the Three Elements",
    "text": "The Interplay of the Three Elements\nThe three elements of credibility, experience, credentials, and social backing, do not exist in isolation. They interact with each other in complex ways to create a strong foundation of credibility.\nFor instance, someone with extensive experience in a field may be awarded an honorary degree, which adds to their credentials and further enhances their social backing. Conversely, someone with strong social backing may be able to leverage their connections to gain valuable work experience, which in turn enhances their credibility. Famous scientists like Neil deGrasse Tyson started with a lot of experience and credentials but now have a lot of social backing too!\nSomeone looking to create a social backing in the form of TikToc, Twitter, or LinkedIn followers might leverage their experience and credentials to boost their influence. How many times have you heard on these platforms, “I’m an x so I know this is the best way to y”? Or “In all my years doing x, I’ve learned that you need to y”? Those influencers are using their other forms of credibility to show their expertise and boost their social backing.\nPersonal projects can also serve as a bridge between the different elements of credibility. For example, a data scientist with no formal credentials but extensive experience in the field may build a personal project that showcases their skills and knowledge. This project can then be used to demonstrate their credentials and enhance their social backing.\nIt’s important to understand the interplay between these three elements of credibility and use them to your advantage. This can involve identifying areas where you need to improve your credibility and then taking concrete steps to address them. It may also involve identifying areas where you have strengths and leveraging them to enhance your credibility even further.\nUltimately, it is the interplay between experience, credentials, and social backing that establishes credibility and creates opportunities for personal and professional growth. Understanding how these elements work together can help you take your career to the next level and achieve your goals."
  },
  {
    "objectID": "posts/2023-03-03-building-credibility/index.html#credibility",
    "href": "posts/2023-03-03-building-credibility/index.html#credibility",
    "title": "Building Credibility: The Key to Success",
    "section": "Credibility",
    "text": "Credibility\nIn conclusion, credibility is an essential factor in establishing a successful career, business, or personal brand. While experience, credentials, and social backing are the three main elements of credibility, the interplay between them is what ultimately determines one’s level of credibility.\nThrough personal anecdotes, we have seen that one can gain credibility in many ways. Whether you are starting out in a new field, looking to advance in your career, or seeking to establish yourself as an expert, developing a strong foundation of credibility is critical.\nBy combining experience, credentials, and social backing in strategic ways, you can establish yourself as a credible authority in your field. Remember that credibility is not just about what you know, but also how you present yourself and your work. Be intentional in your actions, and always strive to build a strong reputation based on trust, respect, and expertise.\nSo, if you’re looking to establish yourself as a credible authority, start by focusing on these three elements and find ways to showcase your knowledge, skills, and achievements. You will find that you can build a strong foundation of credibility that will help you achieve your goals and succeed in your chosen field."
  },
  {
    "objectID": "books/2023-02-21-working-backwards/index.html",
    "href": "books/2023-02-21-working-backwards/index.html",
    "title": "Review of Working Backwards",
    "section": "",
    "text": "Working Backwards by Colin Bryar and Bill Carr is an engaging book that reveals the secrets behind Amazon’s unique and highly successful business practices. The book introduces readers to the concept of “working backwards,” which puts the customer at the forefront of product development and creates a solution that meets their needs. This customer-centric approach has been key to Amazon’s success and is one of the foundational principles discussed in the book.\nThe book is divided into two parts. The first part lays the foundation for the application of Amazon’s practices in the second part. It covers Amazon’s unique philosophies (Leadership Principles) and how they are maintained through various mechanisms, such as the use of bar raisers in the hiring process. Bar raisers are experienced employees who are responsible for maintaining Amazon’s high hiring standards. They conduct a separate interview and provide a second opinion on whether the candidate is a good fit for the company. The idea is to increase the talent density at the company progressively by raising the hiring bar.\nOne of the most impactful concepts in the book is the idea of single threaded leadership, where a project has one person who is responsible for its success. This ensures that there is clear accountability and responsibility. By contrast, the concept of diffusion of responsibility and its impact on group behaviors is well acknowledged in psychology. The larger the responsible group, the less each individual feels responsible for the outcome. Single threaded leadership combats diffusion of responsibility by giving an individual the authority but also the responsibility to ensure success.\nAnother essential concept in the book is the use of six-page narratives to communicate complex information effectively. Amazon uses these to present quarterly and yearly business updates, and the meeting structure is altered to discuss the ideas and answer questions. Senior executives meet and spend the first 20 minutes in silence reviewing the document. The idea is grounded in the belief that the written word is a more effective mode of information transfer than charismatic speaking for a potentially bad idea. These narratives are also available for employees throughout the company, enabling Jeff to stay aligned on all major retail and marketplace programs and give feedback efficiently even as he devoted less time to those businesses.\nKey to the “working backwards” concept is Amazon’s PR/FAQ process. Before any product development starts, the team writes a press release and FAQ document, as if the product is already complete. The purpose is to ensure that the team understands the product and its benefits from the customer’s perspective. This process has helped Amazon to develop products that resonate with customers and avoid building products that don’t meet their needs.\nThe book stresses the importance of tracking controllable input metrics, which measure the inputs into the business, rather than just the outputs. Amazon tracks metrics such as the number of customer reviews and the time it takes to resolve customer issues to identify areas for improvement and continuously refine their business practices.\nIn addition, the book covers Amazon’s approach to operations, which emphasizes automation and continuous improvement. By automating repetitive tasks and streamlining processes, Amazon is able to reduce costs and improve efficiency. Data-driven decision-making, experimentation, and iteration are also key to Amazon’s success.\nOverall, Working Backwards is an insightful read for anyone looking to learn from Amazon’s innovative business practices. The book provides valuable insights into Amazon’s customer-centric approach, unique hiring process, and use of input metrics. I found the concept of PR/FAQs to be particularly impactful, as it forces teams to think through all aspects of the product and its impact on the customer. The book explores both successes and failures, with all foundational practices in the first part of the book being the solution of some failure or another. This perspective and thought process of pivoting after failure is what makes this book such a good read. I highly recommend this book to anyone looking to enhance their business strategy and stay ahead of the competition.\nRating: ⭐⭐⭐⭐⭐"
  },
  {
    "objectID": "books/2022-07-24-the-hard-thing-about-hard-things/index.html",
    "href": "books/2022-07-24-the-hard-thing-about-hard-things/index.html",
    "title": "Review of The Hard Thing About Hard Things",
    "section": "",
    "text": "The book, The Hard Thing About Hard Things, chronicles the journey of Ben Horowitz, a successful entrepreneur, venture capitalist, and co-founder of Andreessen Horowitz. It offers a firsthand account of the struggles he faced while running a startup, including raising capital, selling the company, and managing employees. I had mixed feelings about the book—at times, the book is very insightful but it can also be off-putting. After browsing Amazon reviews, I noticed that people either love or hate this book, and I can understand why.\nHorowitz provides advice on how to navigate the challenges that come with running a business, such as creating a strong company culture, hiring and firing, and leading through uncertainty. But, often the book felt arrogant or braggy, without providing real depth. Many of his stories have examples of how he utilized his network to help solve the problem, but it would have been great to get more actionable advice on how he created those connections in the first place.\nIn fact, the book makes little attempt to empathize with non-executive readers. While motivational for C-Suite executives, for most people the advice extends little beyond offering insights into executive thought processes. I felt concerned that perhaps I was alone in my critiques, but many other Amazon reviewers also felt so as well. I, like others, found myself putting it down at 50%, then at 80%, forging on yet still feeling mixed. And while it was not an issue for me, some readers may find the author’s writing style and language too casual or even offensive at times.\nNow, there were good parts. I particularly liked the “Ones and Twos” section where he compares two types of executives. One’s like making decisions and two’s like to keep things running. The reason I found it insightful is because he argues that successful CEOs and executives have learned to have characteristics of both. His “wartime vs peacetime” CEO analogy was also useful and reminded me of my military analysis courses while at the Air Force Academy. Basically, the best leaders for winning the war, rarely make the best leaders to rebuild.\nIn business, peacetime CEOs work to expand the existing market, whereas wartime CEOs fight to define the market. An example of a peacetime CEO might be John Chambers and for wartime Steve Jobs. He noted that often management books are written for peacetime CEOs with the only exception he found was for Andy Grove’s book Only the Paranoid Survive. On a side note, he makes other great book recommendations such as Built to Last, High Output Management, and Good to Great and explores some of the ideas in these books.\nHe also touches on a menagerie of topics from improving one-on-ones to scaling a company. They are good topics. I can’t decide if he devotes too little time to them or if they are just too obvious to say more. In this vein are topics like holding employees accountable, avoiding what he calls the scale anticipation fallacy, when to hire senior employees–all good topics for new founders or CEOs.\nWhether or not you should read this book depends on your individual needs and preferences. While the book has its strengths, to me it falls short of being an essential business book. The advice provided is not actionable for most readers, but does provide additional perspectives of business. If you are a C-Suite executive looking for insight into the thought process of others in your position, you may benefit from this book. Otherwise, I would recommend exploring other business guides that offer more actionable advice.\nRating: ⭐⭐⭐"
  },
  {
    "objectID": "books/2023-01-12-flawless-consulting/index.html",
    "href": "books/2023-01-12-flawless-consulting/index.html",
    "title": "Review of Flawless Consulting",
    "section": "",
    "text": "Flawless Consulting by Peter Block is written as guide for consultants of all levels, covering the fundamentals of consulting, the process of engagement, and achieving results for clients. The book emphasizes the importance of building trust, understanding client needs, and staying curious and adaptable.\nOne of the strengths of Flawless Consulting is its emphasis on communication and relationship-building. Block’s guidance on how to manage difficult conversations and conflicts, as well as how to build rapport with clients and colleagues, is valuable not only for consultants but also for anyone working in a professional setting. By stressing the importance of active listening, clear and concise communication, and empathy, Block’s advice can help readers improve their interpersonal skills and work more effectively with others.\nWhile many readers may find some of the ideas presented in Flawless Consulting to be intuitive, Block’s guidance on how to engage with clients effectively is helpful. He provides practical advice for achieving positive outcomes, navigating difficulties, and maintaining strong professional relationships. The book’s real-world examples and practical tips make it a valuable resource for both novice and experienced consultants.\nHowever, the book’s wordiness and repetition at times was a drawback for me and made it a bit of a slog to get through some sections. Despite this, I appreciated the useful insights into the consulting profession, the thought process of a consultant, and the value they bring. Towards the end, there is an impactful story of a teacher who changed their approach to teaching after reading an earlier edition of Flawless Consulting. To me, it demonstrates the value of Block’s approach for anyone looking to improve their ability to work with others and achieve their goals.\nOverall, Flawless Consulting is a valuable resource for those interested in consulting or improving their professional skills. It provides guidance on how to avoid common pitfalls, deliver high-quality work that meets client needs, and build and maintain strong client relationships.\nRating: ⭐⭐⭐⭐"
  },
  {
    "objectID": "books/2022-05-03-swipe-to-unlock/index.html",
    "href": "books/2022-05-03-swipe-to-unlock/index.html",
    "title": "Review of Swipe to Unlock",
    "section": "",
    "text": "I recently finished reading Swipe to Unlock by Neel Mehta, Aditya Agashe, and Parth Detroja, and I wanted to share my thoughts on this fascinating book. It’s an excellent read for anyone who wants to gain a better understanding of the technology industry, the role of smartphones in modern society, and the future of technology.\nThe book starts with a comprehensive historical overview of technology’s evolution, from the earliest computers to the advent of smartphones and beyond. The authors delve into the various components that make up a smartphone, including the hardware, software, and sensors, and explain the role of each component and how they work together to create the amazing experience that we all enjoy. While much of the content here was a review for me as a technologist, the explanations were entertaining and easy to read.\nSwipe to Unlock continues by discussing how technology and business strategies have evolved together—all while using case studies to impart the impact these changes have on our daily lives. For instance, the book provides insights into the various business models that have been successful in the technology industry, such as advertising and freemium models. Additionally, the authors also offer valuable advice on how to create a successful startup in this ever-changing field. I found their explanations of the motivations that drive innovation and entrepreneurship to be particularly illuminating.\nTo me, one of the more thought-provoking sections of the book is the discussion of the business interplay between Eastern and Western business strategies. The authors provide a nuanced look into the different approaches taken by companies in these regions and how they have learned from each other to create successful strategies that take into account cultural and historical factors. For instance, the Eastern approach tends to emphasize building long-term relationships with business partners, while Western businesses prioritize scalability, efficiency, and speed. The book offers insights into how businesses can adopt a more collaborative and globally-minded approach by learning from each other’s strengths and weaknesses. Undoubtedly, the East and West will continue to learn from each other and alter the way we do business globally.\nOne of the book’s strengths is its ability to present complex topics in an easy-to-understand format. The authors use real-world examples and analogies to explain difficult concepts, making it easier for readers to follow along. However, some readers may find the book lacking in in-depth analysis on certain topics, which may leave them wanting more.\nOverall, I highly recommend Swipe to Unlock to anyone who wants to gain a better understanding of the technology industry. As someone who works in the tech industry, I found the book to be incredibly insightful and informative. The real-world examples used to illustrate the concepts discussed in the book were particularly engaging. It’s an excellent read for tech enthusiasts, business professionals, and anyone who wants to learn more about how technology impacts our daily lives. So go ahead, swipe right, and add this book to your reading list!\nRating: ⭐⭐⭐⭐⭐"
  },
  {
    "objectID": "books/2022-02-15-lean-startup/index.html",
    "href": "books/2022-02-15-lean-startup/index.html",
    "title": "Review of The Lean Startup",
    "section": "",
    "text": "Back in 2017, a coworker and friend of mine recommended me this book. The Lean Startup is a business book written by Eric Ries that outlines a new approach to starting and growing a business. This approach, called the “lean startup method,” emphasizes the importance of rapid experimentation, continuous improvement, and customer feedback.\nWhile I bought the book right away, I embarrassingly didn’t get around to reading it until years later. It’s a shame I waited so long, as the book has undoubtedly had a profound impact on the way companies approach business. Understanding this shift in thinking has provided valuable insight into some of the decisions that organizations make. I also have a much better understanding of what goes into making a great product.\nThe book starts by discussing the problem with traditional business plans and why they are not suitable for today’s fast-paced and constantly changing business landscape. It then introduces the concept of a “minimum viable product” (MVP) and explains how it can be used to test a business idea quickly and cheaply.\nThe concept of an MVP was not new to me, however this book offered a more nuanced perspective on the subject. Now when I start new data science project, I develop and test initial models more quickly to share them with my team for feedback. This helps us iterate and refine the models faster, and ultimately leads to better outcomes.\nIn addition, I’ve found thinking in terms of MVPs has helped me focus on the most important aspects of a project, rather than getting bogged down in details that don’t add significant value. This has allowed me to be more efficient with my time and resources, and has led to more impactful work.\nThe author then dives into the build-measure-learn feedback loop, which is the core of the lean startup method. This loop involves building a product, measuring its performance, and learning from customer feedback, and then using those learnings to improve the product in the next iteration.\nThe book also covers the importance of focusing on the right metrics and key performance indicators (KPIs), as well as the value of continuous innovation and pivoting when necessary. As someone who works in data science, I found the book’s emphasis on metrics and KPIs to be particularly relevant. Ries stresses the importance of focusing on the right metrics and using them to inform decisions about the product and the business as a whole. This approach aligns with the data-driven mindset that I’ve developed through my work in analytics.\nBut even for those who don’t work in data science, the insights offered by The Lean Startup are valuable. The book’s emphasis on creating a culture of experimentation and continuous improvement can be applied to any business or organization. Ries provides practical strategies for developing and testing ideas, and for pivoting when necessary in order to find the right product-market fit.\nOverall, The Lean Startup is an important book for anyone interested in entrepreneurship or innovation. It offers a practical business approach for creating and improving products that meet the needs of customers. Whether you work in data science or any other field, this book is a must-read for anyone looking to succeed in the world of business.\nRating: ⭐⭐⭐⭐⭐"
  },
  {
    "objectID": "tutorials/2018-05-15-Imbalanced-Classification-with-mlr/index.html",
    "href": "tutorials/2018-05-15-Imbalanced-Classification-with-mlr/index.html",
    "title": "Imbalanced Classification with mlr",
    "section": "",
    "text": "This notebook presents a reference implementation of an imbalanced data problem, namely predicting employee attrition. We’ll use mlr, a package designed to provide an infrastructure for Machine Learning in R. Additionally, there is a companion post which investigates the effectiveness of SMOTE compared to non-SMOTE models.\nUnfortunately, the data is proprietary and we cannot disclose the details of the data with outside parties. But the field represented by this data sees 20% annual turnover in employees. Each employee separation costs roughly $20K. Meaning, a 25% reduction to employee attrition results in an annual savings of over $400K.\nUsing a predictive model, HR organizations can build on the data of today to anticipate the events of tomorrow. This forward notice offers the opportunity to respond by developing a tailored retention strategy to retain employees before they jump ship.\nThis work was part of a one month PoC for an Employee Attrition Analytics project at Honeywell International. I presented this notebook at a Honeywell internal data science meetup group and received permission to post it publicly. I would like to thank Matt Pettis (Managing Data Scientist), Nabil Ahmed (Solution Architect), Kartik Raval (Data SME), and Jason Fraklin (Business SME). Without their mentorship and contributions, this project would not have been possible.\n\n\n\n# Libraries\nlibrary(tidyverse)    # Data manipulation\nlibrary(mlr)          # Modeling framework\nlibrary(parallelMap)  # Parallelization\nlibrary(rpart.plot)   # Decision Tree Visualization\nlibrary(parallel)     # To detect # of cores\n\n# Parallelization\nparallelStartSocket(detectCores())\n\n# Loading Data\nsource(\"prep_EmployeeAttrition.R\")\n\n\n\nSince the primary purpose of this notebook is modeling employee attrition, we won’t go into the data preprocessing steps; but they involved sql querying, reformatting, cleaning, filtering, and variable creation.\nThe loaded data represents a snapshot in time, aggregating 52-weeks of history into performance and summary metrics. To build a predictive model, we choose the end of this 52-week period to be at least 4 weeks in the past. Finally we created a variable indicating if an employee left in the following four week period.\nTo get summary statistics within mlr, you can use summarizeColumns():\nsummarizeColumns(data)\nOutput not shown for proprietary reasons.\n\n\nEmployees: 2852  Model Features: 15  Target Variable: Left4wk \ndata %>%\n  summarise(`Total Employees` = n(),\n            `Attrition Count` = sum(Left4wk==\"Left\"),\n            `Attrition Percent` = mean(Left4wk==\"Left\")) %>% knitr::kable()\n\n\n\nTotal Employees\nAttrition Count\nAttrition Percent\n\n\n\n\n2852\n201\n0.0704769\n\n\n\n\n\n\nConcern: Employee attrition is a imbalanced classification problem, meaning that the group of interest is relatively rare. This can cause models to overclassify the majority group in an effort to get better accuracy. After all, if predict every employee will stay, we can get an accuracy of 93%, but this is not a useful model.  Solution: There are two general methods to overcome this issue: sampling techniques and skew-insensitive classifiers. Synthetic Minority Oversampling TEchnique (SMOTE) is a sampling technique well suited for employee attrition. We’ll use this method to create a balanced model for predicting employee attrition."
  },
  {
    "objectID": "tutorials/2018-05-15-Imbalanced-Classification-with-mlr/index.html#model-development",
    "href": "tutorials/2018-05-15-Imbalanced-Classification-with-mlr/index.html#model-development",
    "title": "Imbalanced Classification with mlr",
    "section": "Model Development",
    "text": "Model Development\nWe’ll use mlr to help us setup the models, run cross-validation, perform hyperparameter tuning, and measure performance of the final models.\n\nModel Setup\n\nDefining the Task\nJust as dplyr provides a grammar for manipulating data frames, mlr provides a grammar for data modeling. The first grammatical object is the task. A task is an object that defines at minimum the data and the target.\nFor this project, our task is to predict employee attrition 4 weeks out. Here we also create a holdout test and train dataset for each task.\n# Defining Task\ntsk_4wk <- makeClassifTask(id = \"4 week prediction\",\n                       data = data %>% select(-c(!! exclude)),\n                       target = \"Left4wk\",  # Must be a factor variable\n                       positive = \"Left\"\n                       )\ntsk_4wk <- mergeSmallFactorLevels(tsk_4wk)\n\n# Creating 4 week holdout datasets\nho_4wk <- makeResampleInstance(\"Holdout\", tsk_4wk, stratify = TRUE)   # Default 1/3rd\ntsk_train_4wk <- subsetTask(tsk_4wk, ho_4wk$train.inds[[1]])\ntsk_test_4wk <- subsetTask(tsk_4wk, ho_4wk$test.inds[[1]])\nNote that the target variable needs to be a factor variable. For Python users, a factor variable is a data type within R specific for representing categorical variables. It can represent information as ordered (e.g. small, medium, large) or unordered (e.g. red, green, blue) and models can take advantage of these relationships. Variables in this dataset were reformatted to factor as part of the preprocessing.\ntrain_target <- table(getTaskTargets(tsk_train_4wk))\ntrain_target\n  Left Stayed\n   134   1767\nAgain, we are dealing with an imbalanced classification problem. After splitting the data, our training sample has 134 employees that left out of 1901 total employees.\nWe’ll use the SMOTE technique described earlier to synthetically generate more employees that left. This will result in a more balanced dataset for training. However, since the test set is still imbalanced, we need to consider balanced performance measures like balanced accuracy and F1 when evaluating and tuning our models.\n\n\nDefining the Learners\nNext, we’ll use three different models to predict employee attrition. The advantage of this approach is that some models perform better on certain problems. By using a few different models were more likely to use a good model for this problem. Also, while some models might provide a better answer, they can be more difficult to explain how or why they work. By using multiple models, we should be able to provide both a predictive and explainable answer. The best of both worlds.\nHere we define the three models we will use to predict employee attrition. Notice they are wrapped in a SMOTE function.\nlrns <- list(\n  makeSMOTEWrapper(makeLearner(\"classif.logreg\", predict.type = \"prob\"),\n                   sw.rate = 18, sw.nn = 8),\n  makeSMOTEWrapper(makeLearner(\"classif.rpart\", predict.type = \"prob\"),\n                   sw.rate = 18, sw.nn = 8),\n  makeSMOTEWrapper(makeLearner(\"classif.randomForest\", predict.type = \"prob\"),\n                   sw.rate = 18, sw.nn = 8))\n\n\nPause: Let’s review the process flow\n\nThe order of operations is important. If you SMOTE before splitting the data, then you’ve effectively polluted the training set with information from the test set! mlr has a smote() function, but that works by redefining the task and will happen before the resampling split. Therefore, we wrapped the smote around the learner which is applied after the resampling split.\n\n\nDefining the Resampling Strategy\nTo ensure extensible models to new data, we’ll use cross-validation to guard against overfitting.\nfolds <- 20\nrdesc <- makeResampleDesc(\"CV\", iters = folds, stratify = TRUE) # stratification with respect to the target\nWe use 20 folds here, but I’d recommend fewer during the exploratory phase since more folds require more computation.\n\n\nModel Cross-validation\nLet’s run a quick cross-validation iteration to see how the models perform before tuning them.\nbchmk <- benchmark(lrns,\n                  tsk_train_4wk,\n                  rdesc, show.info = FALSE,\n                  measures = list(acc, bac, auc, f1))\nbchmk_perf <- getBMRAggrPerformances(bchmk, as.df = TRUE)\nbchmk_perf %>% select(-task.id) %>% knitr::kable()\n\n\n\n\n\n\n\n\n\n\nlearner.id\nacc.test.mean\nbac.test.mean\nauc.test.mean\nf1.test.mean\n\n\n\n\nclassif.logreg.smoted\n0.6500867\n0.7478255\n0.7899618\n0.2610063\n\n\nclassif.rpart.smoted\n0.7428074\n0.7618259\n0.8358594\n0.3027377\n\n\nclassif.randomForest.smoted\n0.8695530\n0.7185079\n0.8700374\n0.3754485\n\n\n\nNot bad; the best model has an accuracy of 87%. By looking at different, sometimes competing, measures we can better gauge the performance of the models. Above we’ve computed accuracy, balanced accuracy, AUC, and F1.\nShown below are boxplots showing the performance measure distribution for each of the 20 cross-validation iterations. All the models seem to perform reasonably well when applied to new data.\nplotBMRBoxplots(bchmk, measure = acc)\n\nHowever when we look at balanced accuracy, we see a performance drop. Balanced accuracy gives equal weight to the relative proportion of each class (left vs stayed) resulting in a more difficult metric.\nplotBMRBoxplots(bchmk, measure = bac)\n\nWith this we’ve built some models, but now we need to refine them. Let’s see if we can improve performance by tuning the hyperparameters.\n\n\n\nTune Hyperparameters\nTuning works by optimizing the cross-validated aggregated performance metric like accuracy or balanced accuracy. This mitigates overfitting because each fold needs to perform reasonable well as to not pull down the aggregation. For this imbalanced data problem, we’ll tune using both F1 score and balanced accuracy.\nThe SMOTE algorithm is defined by the parameters rate and nearest neighbors. Rate defines how much to oversample the minority class. Nearest neighbors defines how many nearest neighbors to consider. For more information about this algorithm check out this post and the original paper. Since SMOTE has tunable hyperparameters, we’ll tune the logistic regression too. In addition, decision trees and randomForests have model specific hyperparameters. If you’re unsure what hyperparameters are tunable, us getParamSet(<learner>) to find out.\n# Logistic\nlogreg_ps <- makeParamSet(\n              makeIntegerParam(\"sw.rate\", lower = 8L, upper = 28L)\n              ,makeIntegerParam(\"sw.nn\", lower = 2L, upper = 8L)\n              )\n# DecisionTree\nrpart_ps <- makeParamSet(\n              makeIntegerParam(\"sw.rate\", lower = 8L, upper = 28L)\n              ,makeIntegerParam(\"sw.nn\", lower = 2L, upper = 8L)\n              ,makeIntegerParam(\"minsplit\",lower = 10L, upper = 50L)\n              ,makeIntegerParam(\"minbucket\", lower = 5L, upper = 70L)\n              ,makeNumericParam(\"cp\", lower = 0.005, upper = .05)\n              )\n# RandomForest\nrandomForest_ps <- makeParamSet(\n              makeIntegerParam(\"sw.rate\", lower = 8L, upper = 28L)\n              ,makeIntegerParam(\"sw.nn\", lower = 2L, upper = 8L)\n              ,makeIntegerParam(\"ntree\", lower = 50L, upper = 600L)\n              ,makeIntegerParam(\"mtry\", lower = 1L, upper = 20L)\n              ,makeIntegerParam(\"nodesize\", lower = 4L, upper = 50L)\n              )\nAfter defining the bounds of each hyperparameter, we define the tuning control to intelligently search the space for an optimal hyperparameter set. Irace and MBO are different methods for optimizing hyperparameters. After tuning each model, we update the learner with the optimal configuration for future training.\n# ctrl = makeTuneControlMBO(budget=200)\nctrl <- makeTuneControlIrace(maxExperiments = 400L)\nlogreg_tr <- tuneParams(lrns[[1]], tsk_train_4wk, rdesc, list(f1), logreg_ps, ctrl)\nlrns[[1]] <- setHyperPars(lrns[[1]], par.vals=logreg_tr$x)\n\nrpart_tr <- tuneParams(lrns[[2]], tsk_train_4wk, rdesc, list(f1), rpart_ps, ctrl)\nlrns[[2]] <- setHyperPars(lrns[[2]], par.vals=rpart_tr$x)\n\nrandomForest_tr <- tuneParams(lrns[[3]], tsk_train_4wk, rdesc, list(f1), randomForest_ps, ctrl)\nlrns[[3]] <- setHyperPars(lrns[[3]], par.vals=randomForest_tr$x)\nIt’s important to know that for each iteration of the tuning process, a full cross-validation resampling of 20 folds occurs.\n\n\nMeasuring Performance\nNow that we’ve tuned our hyperparameters, we need to train on all training data and assess model performance against the holdout. This will give us some idea how the model will perform on new data.\nbchmk <- benchmark(lrns,\n                  tsk_4wk,\n                  ho_4wk, show.info = FALSE,\n                  measures = list(acc, bac, auc, f1))\nbchmk_perf <- getBMRAggrPerformances(bchmk, as.df = TRUE)\nbchmk_perf %>% select(-task.id) %>% knitr::kable()\n\n\n\n\n\n\n\n\n\n\nlearner.id\nacc.test.mean\nbac.test.mean\nauc.test.mean\nf1.test.mean\n\n\n\n\nclassif.logreg.smoted\n0.7108307\n0.8099716\n0.8424056\n0.3107769\n\n\nclassif.rpart.smoted\n0.8422713\n0.7220403\n0.7973847\n0.3421053\n\n\nclassif.randomForest.smoted\n0.8811777\n0.7636591\n0.9047494\n0.4263959\n\n\n\nOne advantage of using mlr’s benchmark() function is that we can create easy comparisons between the three models. Here is the traditional Area Under the Curve (ROC) visualizing one measure of classification performance. The model performs better as the curve stretches towards the upper left thereby maximizing the area.\ndf_4wk <- generateThreshVsPerfData(bchmk,\n            measures = list(fpr, tpr, mmce, ppv, tnr, fnr),\n            task.id = '4 week prediction')\nplotROCCurves(df_4wk) + ggtitle(\"Four week attrition model ROC curves\")\n\nRight now, we are testing the model against the holdout. But after we finish modeling, we’ll train a model using all the data. To understand how well the model integrates new data, we’ll create the learning curve for various measures of performance.\nrs_cv5 <- makeResampleDesc(\"CV\", iters = 5, stratify = TRUE)\nlc_4wk <- generateLearningCurveData(learners = lrns,\n                               task = tsk_4wk,\n                               percs = seq(0.2, 1, by = 0.2),\n                               measures = list(acc, bac, auc, f1),\n                               resampling = rs_cv5,\n                               stratify = TRUE,\n                               show.info = FALSE)\nplotLearningCurve(lc_4wk, facet.wrap.ncol = 2) +\n  ggtitle(\"Four week prediction learning curve\")\n\nThese plots show that the model may benefit from additional data but with decreasing marginal gains. If we want better performance, more data will only help so much–we’ll need better features."
  },
  {
    "objectID": "tutorials/2018-05-15-Imbalanced-Classification-with-mlr/index.html#results",
    "href": "tutorials/2018-05-15-Imbalanced-Classification-with-mlr/index.html#results",
    "title": "Imbalanced Classification with mlr",
    "section": "Results",
    "text": "Results\n\nConfusion Matrices\n\nLogistic\n        predicted\ntrue     Left Stayed -err.-\n  Left     58      9      9\n  Stayed  261    623    261\n  -err.-  261      9    270\n\n      acc       bac        f1\n0.7160883 0.7852114 0.3005181\n\n\nDecision Tree\n        predicted\ntrue     Left Stayed -err.-\n  Left     39     28     28\n  Stayed  122    762    122\n  -err.-  122     28    150\n\n      acc       bac        f1\n0.8422713 0.7220403 0.3421053\n\n\nrandomForest\n        predicted\ntrue     Left Stayed -err.-\n  Left     42     25     25\n  Stayed   83    801     83\n  -err.-   83     25    108\n\n      acc       bac        f1\n0.8864353 0.7664871 0.4375000\nThese results were computed by running each model on the holdout dataset to simulate new data. Therefore, we should expect similar outcomes from a live implemented production model. Since the randomForest performed the best, we’ll use this model to train our production model but we could also create an ensemble using all three."
  },
  {
    "objectID": "tutorials/2018-05-15-Imbalanced-Classification-with-mlr/index.html#interpretability",
    "href": "tutorials/2018-05-15-Imbalanced-Classification-with-mlr/index.html#interpretability",
    "title": "Imbalanced Classification with mlr",
    "section": "Interpretability",
    "text": "Interpretability\nThere are many ways to extract information from the results of a predictive model which could be valuable to the business. One simple way is to simply use the coefficients from the logistic regression to show any linear trends.\nsummary(getLearnerModel(mdl_4wk_logistic, more.unwrap = TRUE))\nOutput not shown for proprietary reasons.\nWe can also use a decision tree to visualize how the model works and potential reasons why people leave.\nrpart.plot(getLearnerModel(mdl_4wk_decisionTree, more.unwrap=TRUE),\n                       extra=104,\n                       box.palette=\"RdGy\",\n                       branch.lty=3,\n                       shadow.col=\"gray\")\nOutput not shown for proprietary reasons.\nFeature importance plots can also provide valuable insight into how models work. The following code uses a method called permutation feature importance which measures the impact of randomly shuffling the values of a feature.\nimpt_4wk <- generateFilterValuesData(tsk_4wk,\n                                     method = \"permutation.importance\",\n                                     imp.learner = lrns[[3]], measure = mmce)\n\nplotFilterValues(impt_4wk) + ggtitle(\"Feature Importance: 4 Week Prediction\")\nOutput not shown for proprietary reasons.\nMany other methods exist to gain interpretability from blackbox models. A few such methods are SHAP and LIME. Additionally, we can feed the results of these models into a clustering algorithm to group similar types of attrition. If distinct groups emerge, we can create profiles and describe what defines each group."
  },
  {
    "objectID": "tutorials/2018-05-15-Imbalanced-Classification-with-mlr/index.html#production-model",
    "href": "tutorials/2018-05-15-Imbalanced-Classification-with-mlr/index.html#production-model",
    "title": "Imbalanced Classification with mlr",
    "section": "Production Model",
    "text": "Production Model\nFinally, we train on all the data to get a model to use on real world data.\nmdl_4wk_final <- train(lrns[[3]], tsk_4wk)\nIf we were to deploy this model, we’d continue by setting up a model monitoring framework. Part of this consists of tests to alert on changes to:\n\nData\n\nContinues to flow properly (both input and output)\nInputs are statistically similar to training data\n\nModel\n\nPerformance\nComputational load (i.e. is the model taking too long to run for the service?)\n\n\nFor a more detailed list of tests for machine learning production systems, check out the paper by Google researchers, “What’s your ML Test Score? A rubric for ML production systems”.\nparallelStop()\nStopped parallelization. All cleaned up."
  },
  {
    "objectID": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#introduction",
    "href": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#introduction",
    "title": "Missing Data Modeled",
    "section": "Introduction",
    "text": "Introduction\nMissing data analysis is often overlooked in the data modeling process, yet it is essential for developing high quality models. Before cleaning, nearly every dataset contains missing data. As we prepare the dataset for modeling, we make various assumptions that impact how the production model will interact in the real-world.\nUnfortunately, we can’t leave these decisions to algorithms. Most learners simply drop any records with missing data. And models that “support” missing data often only perform basic mean/mode imputation. These common but naive approaches to missingness can erode the data leaving it underpowered and/or biased–unable to answer intended questions. Fortunately, missing data analysis has provided us with a framework to overcome these challenges.\nIn this notebook, I provide an overview to missing data analysis. After reading this, you should have a solid understanding of the problem, considerations, and techniques relevant for missing data analysis."
  },
  {
    "objectID": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#example-data",
    "href": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#example-data",
    "title": "Missing Data Modeled",
    "section": "Example data",
    "text": "Example data\nAdmittedly, the iris dataset is overused. But the data isn’t the point. It’s just a vehicle to get where we’re going. In case you haven’t interacted with the Iris dataset before:\n\nIris Dataset\nThere are three types of iris flowers, each with measurements for sepal length, sepal width, petal length, and petal width.\n\nSetosa\nVersicolour\nVirginica\n\nThe dataset contains 50 observations for each flower."
  },
  {
    "objectID": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#understanding-the-problem-defining-success",
    "href": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#understanding-the-problem-defining-success",
    "title": "Missing Data Modeled",
    "section": "Understanding the problem & defining success",
    "text": "Understanding the problem & defining success\nDepending on your experience, you may already understand why missing data is a problem. However, let me formally define the problem and state a few goals which define success.\n\nLoss of power\nStatistical power refers to the amount of data required to reliably make a statistical claim. Simply said, missing data results in less usable data. Occasionally we have data to spare, but even small amounts of missing data can drastically reduce the size of usable data. In many fields, data collection is extremely costly and sometimes cannot be replicated.\nIf we randomly drop 6% of values, then a sample of the Iris dataset might look like this:\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0           NA         0.2  setosa\n3           NA         3.2          1.3         0.2    <NA>\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\nMost models require complete data (no missing values) from which to learn. After removing rows with missing data, we only have 113 out of 150 complete rows. Therefore, even small amounts of missing data (6%) can result in large data loss (24.7%) from a modeling perspective. Generally, a good solution to missing data will maximize available data.\n\n\nBias\nThe second way missing data impacts a dataset is through bias. Consider the following chart. In this case, random elements were dropped from the data if petal length was less than 3.0. Perhaps the data collector had an unconscious bias against short petal iris flowers.\n\nAt first glance, the available case distribution looks similar to the real data. But a common mistake in exploratory data analysis is to consider each variable individually and not compare this to what the model actually uses. If there is a pattern to the missingness, complete-case (only using complete rows) modeling will result in a biased model. At this point we need to ask ourselves why the data is missing. There can be multiple reasons which may require different strategies to address.\nIf we can successfully rule out any patterns to the missingness, then we can assume it is Missing Completely at Random (MCAR). The following chart shows data that is MCAR.\n\nNotice, even though the sample size decreases, the general distribution remains the same. If the data is MCAR, then bias is not a concern; however data is rarely MCAR. Regardless, a good solution for missing data will seek to make bias as small as possible.\n\n\nEstimating uncertainty\nAny method we use will change the shape of the data. Ideally, this altered shape should resemble the real data as closely as possible. But exactly how close is it? Much of statistics is primarily concerned about two things: estimating the magnitude of a statistic (mean, median, variance, etc.) and the probability/likelihood of occurrence. If we replace missing values with an estimate, how do we account for the variability in that estimate? Said differently, how confident are we that our estimates are accurate?\nFor this reason, advanced techniques like Multiple Imputations using Chained Equations (MICE) give a distribution of potential values. This provides an excellent way to correct estimates of uncertainty. However, it also makes modeling more complicated and is not appropriate for all applications. Even so, we should strive for accurate estimates of uncertainty (standard errors, confidence intervals, p-values, etc.).\n\n\nGoals for Missing Data Analysis\nTo summarize, our goals1 are to:\n\nMaximize the use of available information. \nMinimize bias. \nYield good estimates of uncertainty. \n\nThese goals define success for our missing data problem. Of course, we must also balance these goals against any technical constraints of our project. For instance, streaming analytics will need computationally efficient methods which might not ideally solve the missing data but still provide a good enough proximity for the application. If this happens, it’s important to document these model limitations."
  },
  {
    "objectID": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#investigating-missingness",
    "href": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#investigating-missingness",
    "title": "Missing Data Modeled",
    "section": "Investigating missingness",
    "text": "Investigating missingness\nIt’s important to understand why the data is missing. This could mean one or even multiple reasons. This will help us to choose an appropriate method and avoid biasing our dataset.\n\nConsider how the data was collected\nBy understanding more about the data collection process, you’re able to discover potential reasons why data might be missing.\nFor instance, survey data commonly has missing data. Questions might be confusing or entice people to lie. A good solution for this is to interview a subset of respondents and understand their thought process. Perhaps this subset just didn’t know how to interpret the question.\nDepending on the environment, there might be regular events which impact the data. Monthly power generator tests may briefly shut down computers collecting data. By talking with building management, you can get a schedule of events which may impact your data.\nRegardless of the cause, understanding how the data was collected, its consistency, environment, etc. all help you to understand potential sources bias and missing data.\n\n\nVisualizing the problem\nIt is common in Exploratory Data Analysis (EDA) to only visualize what is there, forgetting about what is not. Actually, much of missing data analysis is part of EDA. But how do you visualize what is not there? You look at the pattern!\nHere I’ve used a packaged called VIM to visualize the missingness pattern:\n#install.packages(\"VIM\")\nrequire(VIM)\naggr(iris_mis, col=c('darkred','slategrey'),numbers=TRUE,sortVars=TRUE,\n    labels=names(iris_mis2),cex.axis=.8,gap=3,ylab=c(\"Proportion of Missing Data\",\"Pattern\"))\n\nOn the left, we have a bar chart showing the proportion of missing data for each variable. And on the right, we have can see the pattern of missingness for each combination of variables. For instance, 75.3% of the data is complete with no missing values.\nWhen visualizing missing data, think about why it might be missing missing and how you plan on using the data in a model. Is a variable with high missingness necessary for the model? Will dropping observations with high missingness induce bias or critically impact power? What is the most/least common missingness pattern? Are there any missingness dependencies between variables (e.g. are two variables are always missing together)?\nYou can also look at any correlations between missing and present data. If we looked at a data set of heights and weights, perhaps taller individuals were more likely to have missing weight. If so, visualizing the histogram of height by missing weight frequency may reveal an interesting pattern.\n\n\nThree types of missing data\nWhile researching the data collection process and performing EDA, it helps to understand the three types of missing data. Each type has different implications. To illustrate, imagine that you’re a field investigator collecting household surveys to understand the labor market.\n\nMCAR - Missing Completely At Random\nYou have just finished collecting surveys. On your way back to the office, a stack of surveys take flight on a gust of wind. As you scramble to catch them, some of the responses are obscured.\nThis is a case of Missing Completely At Random (MCAR). We know this because an unrelated outside event resulted in missing data. In other words, the cause of missingness is unrelated to the data. The wind doesn’t care which answers are obscured. It was a completely random accident.\nIf your data is MCAR, great! It’s the easiest of all missingness to handle. MCAR is un-biased missing data. Simple statistic imputation (mean, median, mode, etc.) is a perfectly legitimate method when your data is MCAR. If you have enough data, deleting rows with missingness (complete-case analysis) is also an option. Unfortunately, MCAR is the least common form of missing data.\n\n\nMAR - Missing At Random\nAfter entering the data and exploratory data analysis, you realize that an occupational subgroup has a high rate of missingness concerning their income (e.g. perhaps attorneys prefer not to disclose their income). While not ideal, this missingness is manageable since you are able to use other variables to tease out their likely income. In other words, the missingness is biased in some way explainable by the data.\nThis type of missingness is considered to be Missing At Random (MAR). Ironically, this is not the best name since it is really conditionally missing at random. But it’s historical so it stuck.\n\n\nMNAR - Missing Not At Random\nLastly, suppose larger households omit the number of people living in the household. In this case, the reason for missingness depends on the missing values themselves and not on other variables for which we have information. Another case of MNAR would be if an unmeasured 3rd party is trying to influence the data.\nThis kind of missingness is considered Missing Not At Random (MNAR). MNAR is bias unexplained by current data. Unfortunately this kind of missingness is difficult to test for or solve. Sometimes the best way to overcome MNAR is by collecting additional information.\n\n\nDifference between MAR and MNAR\nOne way to think about the difference between MAR and MNAR missingness is based on available information. Let’s say that we have a dataset with only one variable: Income. If income is missing for given observations, we have no other information from which to make an educated guess. This is MNAR. However, if we collect Occupation, Employed, Education, Age, Experience, etc. we might be able to make a much better guess at income. Now the missing data is MAR. The key difference is whether the bias can be explained by the current data."
  },
  {
    "objectID": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#methods-to-deal-with-missing-data",
    "href": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#methods-to-deal-with-missing-data",
    "title": "Missing Data Modeled",
    "section": "Methods to deal with missing data",
    "text": "Methods to deal with missing data\n\nConsider\nThree things to consider when choosing a method for handling missing data:\n\nWhat is the type and cause of missingness?\n\nMCAR, MAR, or MNAR\nLikely cause(s)\n\nWhat is the shape of the data?\n\nOrdinal, nominal, continuous, discrete, time series, panel etc.\nWhat does the distribution look like?\n\nWhat are my constraints?\n\nSample size (too large, too small)\nStreaming or batch\nTraining time\n\n\nEach method makes different assumptions or is optimized for a particular type of data. Choose the method that fits your data type and problem best. You will likely need to learn a few different methods to handle various types of data. More complicated algorithms like Multiple Imputation using Chained Equations (MICE) or Maximum Likelihood can be complicated to set up and may take too much computation for a given application.\n\n\nMethods\nI’ve listed methods below in order of least to most complicated. For each variable with missingness, consider the methods at the top first. However, if you are conserned about power and bias, then you’ll need to adopt more complicated methods. Finally, if you have many variables with missingness, you may need to use a specialized algorithm which performs multiple imputation.\nNote: Here I make the distinction between replacement, interpolation, and imputation, however they are all forms of imputation (i.e replacing a missing value with an estimated value).\n\nDeletion\n\nList-wise (complete-case analysis)\nPair-wise (available-case analysis)\nObservations or variables with too little information to be useful \n\nReplacement\n\nStatic value: mean, median, mode, constant, zeros, “missing” for categorical variables\nDynamic value: logical rules, LVCF, hot-deck, cold-deck, random\n\nInterpolation\n\nAppropriate for data following a predictable pattern (1, 2, ?, 4, 5…)\nCommon for time-series or spatial data \n\nMissingness Indicator\n\nIndicator variable to denote a replaced/interpolated/imputed missing value. This assumes there is a unobserved reason/pattern for missingness, and if not can induce bias \n\nImputation\n\nSingle imputation uses other features to predict/approximate the correct value of one variable. In reality, this can be any common model used by predicting the missing values (e.g. regression, knn, etc.).\nMultiple imputation imputes multiple variables at the same time. The rational being if multiple variables have biased missingness, then imputing one variable on others would result in biased imputation. These are usually specialized missing data models which iterate over each variable with missingness until convergence (e.g. MICE, Maximum Likelihood, missForest, etc.). \n\nCombination\n\nOccationally, it makes sense to use create a missingness indicator for select variables. Let’s suppose that variable x’s missingness likely represents a group uncaptured by other variables. Then we should create a missingness indicator for variable x and impute missing values for x.\nA dataset will probably require multiple methods. It is common to drop a variable with lots of missingness, then interpolate a date field, before performing multiple imputation on the rest.\n\nKeep in Mind\n\nAll models are wrong, but some are useful\n\n  — famed statistician George Box\nYour solution to missing data is just a model. It’s wrong, but it can be useful. Don’t take it too seriously, but know that it is important."
  },
  {
    "objectID": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#missing-data-packages-in-r",
    "href": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#missing-data-packages-in-r",
    "title": "Missing Data Modeled",
    "section": "Missing data packages in R",
    "text": "Missing data packages in R\nThis is by no means an exhaustive list, but these packages are fairly popular.\n\nImputation\n\nHmisc - General package with functions for single and multiple imputation\nmissForest - Muliple Imputation based on Random Forest algorithm\nMICE - Muliple Imputation using Chained Equations\nAmelia - Imputation for time series data  \n\nTools and visualization\n\nmitools - Tools for multiple imputation of missing data\nVIM - Visualization and Imputation of Missing values\n\n\nIn the following examples, we’ll use the biased Iris dataset (random elements were dropped if petal length was less than 3.0).\n\nExample in Hmisc\n#install.packages(\"Hmisc\")\nrequire(Hmisc)\nimpute() function replaces the missing value with user defined statistical method (default=median | mean | max etc.)\niris_mis$imputed.Petal.Length <-\n              with(iris_mis, impute(Petal.Length, mean))\niris_mis$imputed.Petal.Width <-\n              with(iris_mis, impute(Petal.Width, mean))\nHowever, as we can see below, using mean imputation increases the bias making it difficult to identify Setosa, the species with the smallest petal length.\n\nFor this reason, you should use more advanced methods when dealing with biased missing data. Even unbiased data will benefit depending on your choice of analytic model (decision trees are probably more affected by mean imputation than linear regression).\naregImpute() function creates multiple imputations using additive regression, bootstrapping, and predictive mean matching.\nimputed_aregImpute <- aregImpute(~ Sepal.Length + Sepal.Width\n                        + Petal.Length + Petal.Width + Species,\n                        data = iris_mis, n.impute = 5)\n5 separate imputations to yield good estimates of uncertainty. The literature suggests 20 or more imputations.\nBut for now, we want to pool those imputations to graph the results.\nimputed <- as.data.frame(impute.transcan(imputed_aregImpute,\n                           imputation = 1, data = iris_mis,\n                           list.out = TRUE, pr=FALSE, check=FALSE))\nHere, muliple imputation does a much better job at matching the shape of the original data.\n\n\n\nExample in MICE\nMICE stands for Multivariate Imputation using Chained Equations\n#install.packages(\"mice\")\nrequire(mice)\nimputed_mice <- mice(data = iris_mis[1:5], m = 5,\n                  method = \"pmm\", maxit=50, seed=500)\nIt is one of the more complicated methods, however it does a great job of imputing missing variables.\n\n\n\nExample in missForest\n#install.packages(\"missForest\")\nrequire(missForest)\nmissForest is a relatively newer imputation algorithm and uses an iterative random forest approach to missing data. It accomplishes this by estimating the accuracy of these predictions and adjusts the model. You can also run it in parallel when installing the package doParallel.\nImputing with missForest:\n#install.packages(\"doParallel\")\nrequire(doParallel)\nregisterDoParallel()                    #registering the processor\ngetDoParWorkers()                       #number of processors running\n#vignette(\"gettingstartedParallel\")     #for more information\nimputed_forest <- missForest(iris_mis[1:5], parallelize = \"forest\")\nimputed_forest$OOBerror                 #calling Out Of Bag error\niris_mis.forest <- imputed_forest$ximp\nAs you can see, it also does quite well."
  },
  {
    "objectID": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#conclusion",
    "href": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#conclusion",
    "title": "Missing Data Modeled",
    "section": "Conclusion",
    "text": "Conclusion\nWhile often overlooked, missing data analysis plays an important role in the modeling process. Common but naive approaches often leave datasets underpowered or biased which could impact real-world model performance.\nMissing data analysis is conscerned with making the best use of availible information while minimizing bias and maintaining accurate estimates of uncertainty. To do this, it is important to understand the type of and causes for missingness. We also need to balance the goals of success with the technical constraints of the problem. Afterward, a reasonable approach to handle missing data often becomes clear.\nThere are a variety of methods developed to overcome the problem of missing data. This article highlights a few of them. As this is a growing field, new methods constantly appear."
  },
  {
    "objectID": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#resources",
    "href": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#resources",
    "title": "Missing Data Modeled",
    "section": "Resources",
    "text": "Resources\nThis notebook:\nhttps://github.com/patdmob/Missing-Data\nArticles:\nhttp://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/\nhttp://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/\nChapters:\nhttp://www.statisticalhorizons.com/wp-content/uploads/2012/01/Milsap-Allison.pdf"
  },
  {
    "objectID": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#footnotes",
    "href": "tutorials/2018-07-15-Missing-Data-Modeled/index.html#footnotes",
    "title": "Missing Data Modeled",
    "section": "Footnotes",
    "text": "Footnotes\n1 These goals are adopted from Paul Allison’s chapter on missing data referenced in the resources section. ↩︎"
  },
  {
    "objectID": "tutorials/2018-05-16-Vanilla-vs-SMOTE/index.html",
    "href": "tutorials/2018-05-16-Vanilla-vs-SMOTE/index.html",
    "title": "Vanilla vs SMOTE Flavored Imbalanced Classification",
    "section": "",
    "text": "This is a companion notebook to Imbalanced Classification with mlr. In this notebook, we investigate whether SMOTE actually improves model performance. For clarity, non-SMOTE models are referred to as “vanilla” models. We compare these two flavors (vanilla and SMOTE) using logistic regression, decision trees, and randomForest. We also consider how tuning model operating thresholds and tuning SMOTE parameters impact the results.\nIf you must know, I had to make this. We kept debating on the effectiveness of techniques like SMOTE during my lunch break. Eventually, my curiosity won out and here we are. Does SMOTE work? Keep reading to find out! Or just skip to the conclusion.\nFor more information about this algorithm, check out the original paper. Or if you’re looking for a visual explanation, this post does a good job.\nThe findings in this notebook represent observed trends but actual results may vary. Additionally, different datasets may respond differently to SMOTE. These findings are not verified by the FDA. ;)\nThis work was part of a one month PoC for an Employee Attrition Analytics project at Honeywell International. I presented this notebook at a Honeywell internal data science meetup group and received permission to post it publicly. I would like to thank Matt Pettis (Managing Data Scientist), Nabil Ahmed (Solution Architect), Kartik Raval (Data SME), and Jason Fraklin (Business SME). Without their mentorship and contributions, this project would not have been possible.\n\n\nThere are lots of performance measures to choose from for classification problems. We’ll look at a few to compare these models.\n\n\nis the percentage of correctly classified instances. However, if the majority class makes up 99% of the data, then it is easy to get an accuracy of 99% by always predicting the majority class. For this reason, accuracy is not a good measure for imbalanced classification problems. 1 - ACC results in the misclassification error or error rate.\n\n\n\n\non the other hand, gives equal weight to the relative proportions of negative and positive class instances. If a model predicts only one class, the best balanced accuracy it could receive is 50%. 1 - BAC results in the balanced error rate.\n\n\n\n\nis the harmonic mean of precision and recall. A perfect model has a precision and recall of 1 resulting in an F1 score of 1. For all other models, there exists a tradeoff between precision and recall. F1 is a measure that helps us to judge how much of the tradeoff is worthwhile.\n\nor"
  },
  {
    "objectID": "tutorials/2018-05-16-Vanilla-vs-SMOTE/index.html#setup",
    "href": "tutorials/2018-05-16-Vanilla-vs-SMOTE/index.html#setup",
    "title": "Vanilla vs SMOTE Flavored Imbalanced Classification",
    "section": "Setup",
    "text": "Setup\n# Libraries\nlibrary(tidyverse)    # Data manipulation\nlibrary(mlr)          # Modeling framework\nlibrary(parallelMap)  # Parallelization  \n\n# Parallelization\nparallelStartSocket(parallel::detectCores())\n\n# Loading Data\nsource(\"prep_EmployeeAttrition.R\")\n\nDefining the Task\nAs before, we define the Task at hand: predicting attrition up to four weeks out.\ntsk_4wk = makeClassifTask(id = \"4 week prediction\",\n                       data = data %>% select(-c(!! exclude)),\n                       target = \"Left4wk\",  # Must be a factor variable\n                       positive = \"Left\"\n                       )\ntsk_4wk <- mergeSmallFactorLevels(tsk_4wk)\nset.seed(5456)\nho_4wk <- makeResampleInstance(\"Holdout\", tsk_4wk, stratify = TRUE)   # Default 1/3rd\ntsk_train_4wk <- subsetTask(tsk_4wk, ho_4wk$train.inds[[1]])\ntsk_test_4wk <- subsetTask(tsk_4wk, ho_4wk$test.inds[[1]])\n\n\nDefining the Learners\nHere we define 3 separate learner lists. Each contains the model with and without SMOTE.\nrate <- 18\nneighbors <- 5\n\nlogreg_lrns = list(\n  makeLearner(\"classif.logreg\", predict.type = \"prob\")\n  ,makeSMOTEWrapper(makeLearner(\"classif.logreg\", predict.type = \"prob\"),\n                   sw.rate = rate, sw.nn = neighbors))\nrpart_lrns = list(\n  makeLearner(\"classif.rpart\", predict.type = \"prob\")\n  ,makeSMOTEWrapper(makeLearner(\"classif.rpart\", predict.type = \"prob\"),\n                   sw.rate = rate, sw.nn = neighbors))\nrandomForest_lrns = list(\n  makeLearner(\"classif.randomForest\", predict.type = \"prob\")\n  ,makeSMOTEWrapper(makeLearner(\"classif.randomForest\", predict.type = \"prob\"),\n                   sw.rate = rate, sw.nn = neighbors))\n\n\nDefining the Resampling Strategy\nHere we define the resampling technique. This strategy is implemented repeatedly throughout this notebook. Each time it chooses different records for each fold accounting for some of the variability between the models.\n# Define the resampling technique\nfolds = 20\nrdesc = makeResampleDesc(\"CV\", iters = folds, stratify = TRUE) # stratification with respect to the target"
  },
  {
    "objectID": "tutorials/2018-05-16-Vanilla-vs-SMOTE/index.html#benchmarking-logistic-regression",
    "href": "tutorials/2018-05-16-Vanilla-vs-SMOTE/index.html#benchmarking-logistic-regression",
    "title": "Vanilla vs SMOTE Flavored Imbalanced Classification",
    "section": "Benchmarking Logistic Regression",
    "text": "Benchmarking Logistic Regression\nFirst, we’ll consider the logistic regression and evaluate how SMOTE impacts model performance.\n# Fit the model\nlogreg_bchmk = benchmark(logreg_lrns,\n                  tsk_train_4wk,\n                  rdesc, show.info = FALSE,\n                  measures = list(acc, bac, auc, f1))\nlogreg_bchmk_perf <- getBMRAggrPerformances(logreg_bchmk, as.df = TRUE)\nlogreg_bchmk_perf %>% select(-task.id) %>% knitr::kable()\n\n\n\n\n\n\n\n\n\n\nlearner.id\nacc.test.mean\nbac.test.mean\nauc.test.mean\nf1.test.mean\n\n\n\n\nclassif.logreg\n0.9290151\n0.4997191\n0.8398939\n0.0000000\n\n\nclassif.logreg.smoted\n0.6743728\n0.7880844\n0.8234845\n0.2869752\n\n\n\nBoth models have nearly an identical AUC value of about 0.83. It seems these models effectively trading off accuracy and balanced accuracy. The SMOTE model has a higher balanced accuracy and F1 score of 78.8% and 0.29 respectively (compared to 50% and 0). And the vanilla model has a higher accuracy of 92.9% (compared to 67.4%).\n# Visualize results\nlogreg_df_4wk = generateThreshVsPerfData(logreg_bchmk,\n            measures = list(fpr, tpr, mmce, bac, ppv, tnr, fnr, f1))\n\nROC Curves\nplotROCCurves(logreg_df_4wk)\n\nLooking at the ROC curves, we see that they intersect but otherwise have similar performance. It is important to note, in practice, we choose a threshold to operate a model. Therefore, the model with a larger area may not be the model with better performance within a limited threshold range.\n\n\nPrecision-Recall Curves\nplotROCCurves(logreg_df_4wk, measures = list(tpr, ppv), diagonal = FALSE)\n\nHere, if you are considering AUC-PR, the vanilla logistic regression does better than the SMOTEd model. Another thing to note is that the positive predictive value (precision) is fairly low for both models. Even though the AUC looked decent at 0.83 there is still a lot of imprecision in these models. Otherwise, the SMOTE model generally does better when recall (TPR) is high and vice verse for the vanilla model.\n\n\nThreshold Plots\nplotThreshVsPerf(logreg_df_4wk,  measures = list(fpr, fnr))\n\nThreshold plots are common visualizations that help determine an appropriate threshold on which to operate. The FPR and FNR clearly illustrate the opposing tradeoff of each model. However it is difficult to compare these models using FPR and FNR since the imbalanced nature of the data has effectively squished the vanilla logistic model to the far left: slope is zero when the threshold is greater than ≈ 0.4.\nplotThreshVsPerf(logreg_df_4wk,  measures = list(f1, bac))\n\nFor our use case, threshold plots for F1 score and balanced accuracy make it easier to identify good thresholds. And while the vanilla logistic regression is still squished to the left, we can compare the performance peaks for the models. For F1, the vanilla model tends to have a higher peak. Whereas for balanced accuracy, SMOTE tends to have a slightly higher peak. Notice that the balanced accuracy for the SMOTEd model centers around the default threshold of 0.5 whereas the F1 score does not.\n\n\nConfusion Matrices\nTo calculate the confusion matrices, we’ll train a new model using the full training set and predict against the holdout. Before, we only used the training data and aggregated the performance of the 20 cross-validated folds. We separate the data this way to prevent biasing our operating thresholds for these models.\nThe training set and holdout are defined at the beginning of this notebook and do not change. However, after tuning the SMOTE parameters, we rerun the cross-validation which may result in changes to the SMOTE model and operating thresholds for both models.\n\nVanilla Logistic Regression (default threshold)\n        predicted\ntrue     Left Stayed -err.-\n  Left      0     67     67\n  Stayed    0    884      0\n  -err.-    0     67     67\n\n      acc       bac       auc        f1\n0.9295478 0.5000000 0.8041298 0.0000000\nIf you just look at accuracy (93%), this model performs great! But it is useless for the business. This model predicted that 0 employees would leave in the next 4 weeks but actually 67 left. This is why we need balanced performance measures like balanced accuracy for imbalanced classification problems. The balanced accuracy of 50% clearly illustrates the problem of this model.\n\n\nSMOTE Logistic Regression (default threshold)\n        predicted\ntrue     Left Stayed -err.-\n  Left     56     11     11\n  Stayed  277    607    277\n  -err.-  277     11    288\n\n      acc       bac       auc        f1\n0.6971609 0.7612362 0.8177382 0.2800000\nThis is the first evidence that SMOTE works. We have a more balanced model (76.1% balanced accuracy compared to 50%) that might actually be useful for the business. It narrows the pool of employees at risk of attrition from 951 down to 333 while capturing 83.6% of employees that actually left. If this were the only information available, then SMOTE does appear to result in a better model.\nHowever the AUC is similar for both models indicating similar performance. As mentioned earlier, we can operate these models at different thresholds.\n\n\nTuning the Operating Threshold\nThe following code tunes the operating threshold for each model:\nmetric <- f1\nlogreg_thresh_vanilla <- tuneThreshold(\n                             getBMRPredictions(logreg_bchmk\n                                  ,learner.ids =\"classif.logreg\"\n                                  ,drop = TRUE)\n                             ,measure = metric)\nlogreg_thresh_SMOTE <- tuneThreshold(\n                             getBMRPredictions(logreg_bchmk\n                                  ,learner.ids =\"classif.logreg.smoted\"\n                                  ,drop = TRUE)\n                             ,measure = metric)\nHere we’ve tuned these models using the F1 measure but we could have easily used a different metric.\n\n\nVanilla Logistic Regression (tuned threshold)\n        predicted\ntrue     Left Stayed -err.-\n  Left     29     38     38\n  Stayed  130    754    130\n  -err.-  130     38    168\n\n      acc       bac       auc        f1\n0.8233438 0.6428885 0.8041298 0.2566372\n\n\nSMOTE Logistic Regression (tuned threshold)\n        predicted\ntrue     Left Stayed -err.-\n  Left     39     28     28\n  Stayed  164    720    164\n  -err.-  164     28    192\n\n      acc       bac       auc        f1\n0.7981073 0.6982846 0.8177382 0.2888889\nSetting the tuned operating threshold results in two very similar models! Depending on the run, there might be a slight benefit to the SMOTEd model, but not enough to say with confidence.\nBut perhaps SMOTE just needs some tuning.\n\n\nTuning SMOTE\nThe SMOTE algorithm is defined by the parameters rate and nearest neighbors. Rate defines how much to oversample the minority class. Nearest neighbors defines how many nearest neighbors to consider. Tuning these should result in better model performance.\nlogreg_ps = makeParamSet(\n              makeIntegerParam(\"sw.rate\", lower = 8L, upper = 28L)\n              ,makeIntegerParam(\"sw.nn\", lower = 2L, upper = 8L)\n              )\nctrl = makeTuneControlIrace(maxExperiments = 400L)\nlogreg_tr = tuneParams(logreg_lrns[[2]], tsk_train_4wk, rdesc, list(f1, bac), logreg_ps, ctrl)\nlogreg_lrns[[2]] = setHyperPars(logreg_lrns[[2]], par.vals=logreg_tr$x)\n\n# Fit the model\nlogreg_bchmk = benchmark(logreg_lrns,\n                  tsk_train_4wk,\n                  rdesc, show.info = FALSE,\n                  measures = list(acc, bac, auc, f1))\nlogreg_thresh_vanilla <- tuneThreshold(\n                                  getBMRPredictions(logreg_bchmk\n                                                    ,learner.ids =\"classif.logreg\"\n                                                    ,drop = TRUE),\n                                  measure = metric)\nlogreg_thresh_SMOTE <- tuneThreshold(\n                                  getBMRPredictions(logreg_bchmk\n                                                    ,learner.ids =\"classif.logreg.smoted\"\n                                                    ,drop = TRUE),\n                                  measure = metric)\n\n\nVanilla Logistic Regression (tuned threshold)\n        predicted\ntrue     Left Stayed -err.-\n  Left     35     32     32\n  Stayed  145    739    145\n  -err.-  145     32    177\n\n      acc       bac       auc        f1\n0.8138801 0.6791805 0.8041298 0.2834008\n\n\nSMOTE Logistic Regression (tuned threshold and SMOTE)\n        predicted\ntrue     Left Stayed -err.-\n  Left     42     25     25\n  Stayed  182    702    182\n  -err.-  182     25    207\n\n      acc       bac       auc        f1\n0.7823344 0.7104917 0.8105795 0.2886598\nIf we account for resampling variance, the tuned SMOTE makes little difference. Perhaps the initial SMOTE parameters close enough to the optimal settings. This table shows how they changed:\n\n\n\n\nRate\nNearest Neighbors\n\n\n\n\nInitial\n18\n5\n\n\nTuned\n9\n2\n\n\n\nThe rate decreased by 9 and the number of nearest neighbors decreased by 3.\nAfter running this code multiple times, SMOTE generally produces models with higher balanced accuracy but lower accuracy. In terms of AUC and F1, it is harder to tell. Either way, even if SMOTE is tuned, observed performance increases are small compared to a vanilla logistic model with a tuned operating threshold. These results may also depend on the data itself. A different dataset intended to solve another imbalanced classification problem may have different results using SMOTE with logistic regression."
  },
  {
    "objectID": "tutorials/2018-05-16-Vanilla-vs-SMOTE/index.html#benchmarking-decision-tree",
    "href": "tutorials/2018-05-16-Vanilla-vs-SMOTE/index.html#benchmarking-decision-tree",
    "title": "Vanilla vs SMOTE Flavored Imbalanced Classification",
    "section": "Benchmarking Decision Tree",
    "text": "Benchmarking Decision Tree\n# Fit the model\nrpart_bchmk = benchmark(rpart_lrns,\n                  tsk_train_4wk,\n                  rdesc, show.info = FALSE,\n                  measures = list(acc, bac, auc, f1))\nrpart_bchmk_perf <- getBMRAggrPerformances(rpart_bchmk, as.df = TRUE)\nrpart_bchmk_perf %>% select(-task.id) %>% knitr::kable()\n\n\n\n\n\n\n\n\n\n\nlearner.id\nacc.test.mean\nbac.test.mean\nauc.test.mean\nf1.test.mean\n\n\n\n\nclassif.rpart\n0.9290539\n0.5888060\n0.6995163\n0.2594619\n\n\nclassif.rpart.smoted\n0.7491146\n0.7771476\n0.8605376\n0.3113137\n\n\n\nLet’s be honest, the SMOTEd logistic regression was lackluster. But for the decision tree model, SMOTE increases AUC by 0.16. Both flavors have similar F1 scores; otherwise we see the same tradeoff between accuracy and balanced accuracy as in the logistic regression.\n# Visualize results\nrpart_df_4wk = generateThreshVsPerfData(rpart_bchmk,\n            measures = list(fpr, tpr, mmce, bac, ppv, tnr, fnr, f1))\n\nROC Curves\nplotROCCurves(rpart_df_4wk)\n\nIt’s easy to see that SMOTE has a higher AUC than the vanilla model, but since the lines cross, each perform better within certain operating thresholds.\n\n\nPrecision-Recall Curves\nplotROCCurves(rpart_df_4wk, measures = list(tpr, ppv), diagonal = FALSE)\n\nThe vanilla model scores much higher on precision (PPV) but declines much more quickly as recall increases. SMOTE is more precise when recall (TPR) is greater than ≈ 0.75. Additionally, notice the straight lines, likely, there are no data in these regions making each model only viable for half the PR Curve.\n\n\nThreshold Plots\nplotThreshVsPerf(rpart_df_4wk,  measures = list(fpr, fnr))\n\nThe nearly vertical slopes of these threshold plots represent the straight lines on the PR Curve plot.\nplotThreshVsPerf(rpart_df_4wk,  measures = list(f1, bac))\n\nIf we’re concerned primarily with balanced accuracy, SMOTE is clearly better at all thresholds. For the F1 score however, it depends on the operating threshold of the model. Notice balanced accuracy is once again centered around the default threshold of 0.5 and the F1 measure is not. The F1 performance to threshold pattern is roughly opposite for the two flavors of decision trees.\n\n\nConfusion Matrices\n\nVanilla Decision Tree (default threshold)\n        predicted\ntrue     Left Stayed -err.-\n  Left     10     57     57\n  Stayed    7    877      7\n  -err.-    7     57     64\n\n      acc       bac       auc        f1\n0.9327024 0.5706676 0.7061694 0.2380952\nUsing the default threshold, the vanilla decision tree manages to identify some employee attrition. In face, its accuracy of 93.3% is higher than the baseline case of always predicting the majority class (93%). It is relatively precise (0.59) but has low recall (0.15). Overall accuracy is high (93.3%), but the model is not very balanced (57.1%).\n\n\nSMOTE Decision Tree (default threshold)\n        predicted\ntrue     Left Stayed -err.-\n  Left     55     12     12\n  Stayed  232    652    232\n  -err.-  232     12    244\n\n      acc       bac       auc        f1\n0.7434280 0.7792260 0.8338117 0.3107345\nThe SMOTE Decision Tree does a much better job of capturing employees that left (82% compared to 15%) but at the cost of precision. The model identifies 287 when only 55 from that group actually leave. Still this model is more useful to the business than the vanilla decision tree at the default threshold.\n\n\nTuning the Operating Threshold\nrpart_thresh_vanilla <- tuneThreshold(\n                                  getBMRPredictions(rpart_bchmk\n                                                    ,learner.ids =\"classif.rpart\"\n                                                    ,drop = TRUE),\n                                  measure = metric)\nrpart_thresh_SMOTE <- tuneThreshold(\n                                  getBMRPredictions(rpart_bchmk\n                                                    ,learner.ids =\"classif.rpart.smoted\"\n                                                    ,drop = TRUE),\n                                  measure = metric)\nAs before, we’ll be using the F1 measure.\n\n\nVanilla Decision Tree (tuned threshold)\n        predicted\ntrue     Left Stayed -err.-\n  Left     22     45     45\n  Stayed   26    858     26\n  -err.-   26     45     71\n\n      acc       bac       auc        f1\n0.9253417 0.6494732 0.7061694 0.3826087\nOnce we tune the threshold, the vanilla decision tree model performs much better–identifying more employees that leave with relatively high precision. The F1 score increases from 0.238 to 0.383.\n\n\nSMOTE Decision Tree (tuned threshold)\n        predicted\ntrue     Left Stayed -err.-\n  Left     32     35     35\n  Stayed   80    804     80\n  -err.-   80     35    115\n\n      acc       bac       auc        f1\n0.8790747 0.6935571 0.8338117 0.3575419\nChanging the operating threshold for the SMOTEd decision tree results in a 13.6% higher accuracy, 8.57% lower balanced accuracy, and higher F1 measure of 4.68%.\nThese changes to the operating threshold result in a similar F1 performance for both flavors of decision tree (0.358 compared to 0.383).\n\n\nTuning SMOTE\nrpart_ps = makeParamSet(\n              makeIntegerParam(\"sw.rate\", lower = 8L, upper = 28L)\n              ,makeIntegerParam(\"sw.nn\", lower = 2L, upper = 8L)\n              )\nctrl = makeTuneControlIrace(maxExperiments = 200L)\nrpart_tr = tuneParams(rpart_lrns[[2]], tsk_train_4wk, rdesc, list(f1, bac), rpart_ps, ctrl)\nrpart_lrns[[2]] = setHyperPars(rpart_lrns[[2]], par.vals=rpart_tr$x)\n# Fit the model\nrpart_bchmk = benchmark(rpart_lrns,\n                  tsk_train_4wk,\n                  rdesc, show.info = FALSE,\n                  measures = list(acc, bac, auc, f1))\nrpart_thresh_vanilla <- tuneThreshold(\n                                  getBMRPredictions(rpart_bchmk\n                                                    ,learner.ids =\"classif.rpart\"\n                                                    ,drop = TRUE),\n                                  measure = metric)\nrpart_thresh_SMOTE <- tuneThreshold(\n                                  getBMRPredictions(rpart_bchmk\n                                                    ,learner.ids =\"classif.rpart.smoted\"\n                                                    ,drop = TRUE),\n                                  measure = metric)\n\n\nVanilla Decision Tree (tuned threshold)\n        predicted\ntrue     Left Stayed -err.-\n  Left     23     44     44\n  Stayed   35    849     35\n  -err.-   35     44     79\n\n      acc       bac       auc        f1\n0.9169295 0.6518454 0.7061694 0.3680000\n\n\nSMOTE Decision Tree (tuned threshold and SMOTE)\n        predicted\ntrue     Left Stayed -err.-\n  Left     32     35     35\n  Stayed   80    804     80\n  -err.-   80     35    115\n\n      acc       bac       auc        f1\n0.8790747 0.6935571 0.7885628 0.3575419\nTuning SMOTE for the decision tree changed the accuracy from 87.9% to 87.9% and the balanced accuracy from 69.4% to 69.4%. The following table shows how the rate and number of nearest neighbors changed:\n\n\n\n\nRate\nNearest Neighbors\n\n\n\n\nInitial\n18\n5\n\n\nTuned\n15\n7\n\n\n\nThe rate decreased by 3 and the number of nearest neighbors increased by 2.\nGiven our data, SMOTE for decision trees seems to offer real performance increases to the model. That said, the performance increases are largely via tradeoff between accuracy and balanced accuracy. Setting the operating threshold for the vanilla model results in a similarly performant model. However, we need to consider that identifying rare events is our primary concern. SMOTE allows us to operate with increased performance when high recall is important."
  },
  {
    "objectID": "tutorials/2018-05-16-Vanilla-vs-SMOTE/index.html#benchmarking-randomforest",
    "href": "tutorials/2018-05-16-Vanilla-vs-SMOTE/index.html#benchmarking-randomforest",
    "title": "Vanilla vs SMOTE Flavored Imbalanced Classification",
    "section": "Benchmarking randomForest",
    "text": "Benchmarking randomForest\n# Fit the model\nrandomForest_bchmk = benchmark(randomForest_lrns,\n                  tsk_train_4wk,\n                  rdesc, show.info = FALSE,\n                  measures = list(acc, bac, auc, f1))\nrandomForest_bchmk_perf <- getBMRAggrPerformances(randomForest_bchmk, as.df = TRUE)\nrandomForest_bchmk_perf %>% select(-task.id) %>% knitr::kable()\n\n\n\n\n\n\n\n\n\n\nlearner.id\nacc.test.mean\nbac.test.mean\nauc.test.mean\nf1.test.mean\n\n\n\n\nclassif.randomForest\n0.9310609\n0.5878136\n0.8942103\n0.2674242\n\n\nclassif.randomForest.smoted\n0.8868540\n0.7553178\n0.8875052\n0.4339340\n\n\n\nFor the randomForest models, we see similar patters as for the logistic regression and decision tress. There is a trade off between accuracy and balanced accuracy. Unlike the decision tree models, SMOTE does not improve AUC for SMOTE randomForest (both are ≈ 0.89).\n# Visualize results\nrandomForest_df_4wk = generateThreshVsPerfData(randomForest_bchmk,\n            measures = list(fpr, tpr, mmce, bac, ppv, tnr, fnr, f1))\n\nROC Curves\nplotROCCurves(randomForest_df_4wk)\n\nBoth models cross multiple times showing either model is likely good for most thresholds.\n\n\nPrecision-Recall Curves\nplotROCCurves(randomForest_df_4wk, measures = list(tpr, ppv), diagonal = FALSE)\n\nThis PR-Curve shows more distinctly that SMOTE generally performs better when recall is high, whereas the vanilla model generally performs better when recall is lower.\n\n\nThreshold Plots\nplotThreshVsPerf(randomForest_df_4wk,  measures = list(fpr, fnr))\n\nplotThreshVsPerf(randomForest_df_4wk,  measures = list(f1, bac))\n\nInterestingly, the SMOTE randomForest does not center balanced accuracy around the default threshold; rather F1 is centered on the 0.5 threshold. Otherwise we see that SMOTE produces a higher peak for balanced accuracy but lower for F1. Additionally, the vanilla model is still squished to the left due to its class imbalance.\n\n\nConfusion Matrices\n\nVanilla randomForest (default threshold)\n        predicted\ntrue     Left Stayed -err.-\n  Left     15     52     52\n  Stayed    7    877      7\n  -err.-    7     52     59\n\n      acc       bac       auc        f1\n0.9379600 0.6079810 0.8559718 0.3370787\nAs far as vanilla models go, and given the default threshold, the randomForest performs the best. That said, this model captures very few employees that leave.\n\n\nSMOTE randomForest (default threshold)\n        predicted\ntrue     Left Stayed -err.-\n  Left     34     33     33\n  Stayed   78    806     78\n  -err.-   78     33    111\n\n      acc       bac       auc        f1\n0.8832808 0.7096137 0.8686263 0.3798883\nThe SMOTEd randomForest also does well. The accuracy is high and manages a good balanced accuracy.\n\n\nTuning the Operating Threshold\nrandomForest_thresh_vanilla <- tuneThreshold(\n                                  getBMRPredictions(randomForest_bchmk\n                                                    ,learner.ids =\"classif.randomForest\"\n                                                    ,drop = TRUE),\n                                  measure = metric)\nrandomForest_thresh_SMOTE <- tuneThreshold(\n                                  getBMRPredictions(randomForest_bchmk\n                                                    ,learner.ids =\"classif.randomForest.smoted\"\n                                                    ,drop = TRUE),\n                                  measure = metric)\nAs before, we’ll be using the F1 measure.\n\n\nVanilla randomForest (tuned threshold)\n        predicted\ntrue     Left Stayed -err.-\n  Left     38     29     29\n  Stayed   78    806     78\n  -err.-   78     29    107\n\n      acc       bac       auc        f1\n0.8874869 0.7394644 0.8559718 0.4153005\n\n\nSMOTE randomForest (tuned threshold)\n        predicted\ntrue     Left Stayed -err.-\n  Left     36     31     31\n  Stayed   81    803     81\n  -err.-   81     31    112\n\n      acc       bac       auc        f1\n0.8822292 0.7228422 0.8686263 0.3913043\nAt the tuned threshold, the performance of both flavors perform better and are once again very similar. Depending on the run, SMOTE will have a higher balanced accuracy, but otherwise there is little difference between the models.\n\n\nTuning SMOTE\nrandomForest_ps = makeParamSet(\n              makeIntegerParam(\"sw.rate\", lower = 8L, upper = 28L)\n              ,makeIntegerParam(\"sw.nn\", lower = 2L, upper = 8L)\n              )\nctrl = makeTuneControlIrace(maxExperiments = 200L)\nrandomForest_tr = tuneParams(randomForest_lrns[[2]], tsk_train_4wk, rdesc, list(f1, bac), randomForest_ps, ctrl)\nrandomForest_lrns[[2]] = setHyperPars(randomForest_lrns[[2]], par.vals=randomForest_tr$x)\n# Fit the model\nrandomForest_bchmk = benchmark(randomForest_lrns,\n                  tsk_train_4wk,\n                  rdesc, show.info = FALSE,\n                  measures = list(acc, bac, auc, f1))\nrandomForest_thresh_vanilla <- tuneThreshold(\n                                  getBMRPredictions(randomForest_bchmk\n                                                    ,learner.ids =\"classif.randomForest\"\n                                                    ,drop = TRUE),\n                                  measure = metric)\nrandomForest_thresh_SMOTE <- tuneThreshold(\n                                  getBMRPredictions(randomForest_bchmk\n                                                    ,learner.ids =\"classif.randomForest.smoted\"\n                                                    ,drop = TRUE),\n                                  measure = metric)\n\n\nVanilla randomForest (tuned threshold)\n        predicted\ntrue     Left Stayed -err.-\n  Left     40     27     27\n  Stayed  101    783    101\n  -err.-  101     27    128\n\n      acc       bac       auc        f1\n0.8654048 0.7413808 0.8527470 0.3846154\n\n\nSMOTE randomForest (tuned threshold and SMOTE)\n        predicted\ntrue     Left Stayed -err.-\n  Left     37     30     30\n  Stayed   98    786     98\n  -err.-   98     30    128\n\n      acc       bac       auc        f1\n0.8654048 0.7206895 0.8637384 0.3663366\nGiven this data, tuning SMOTE does not seem to improve performance. The following table shows how the parameters for SMOTE changed during the tuning process:\n\n\n\n\nRate\nNearest Neighbors\n\n\n\n\nInitial\n18\n5\n\n\nTuned\n18\n3\n\n\n\nThe rate did not change and the number of nearest neighbors decreased by 2.\nGiven this data for randomForest, SMOTE does little to improve model performance. At optimized operating thresholds, both flavors end up with very similar accuracy and balanced accuracy. There does appear to be some benefit using SMOTE where recall is high and precision is low, however the business may not want to throw such a large net in order to capture all of the employees that leave. Practically speaking, SMOTE did not improve the performance for this problem when using randomForest.\nparallelStop()\nStopped parallelization. All cleaned up."
  },
  {
    "objectID": "tutorials/2018-05-16-Vanilla-vs-SMOTE/index.html#conclusion",
    "href": "tutorials/2018-05-16-Vanilla-vs-SMOTE/index.html#conclusion",
    "title": "Vanilla vs SMOTE Flavored Imbalanced Classification",
    "section": "Conclusion",
    "text": "Conclusion\nGiven this data, SMOTE improved AUC of the decision tree model but offered little improvement for logistic regression or randomForest. Otherwise, SMOTE offered a way to trade accuracy for balanced accuracy. For our problem of employee attrition, this trade off is worth it to continue using SMOTE. Even when operating thresholds are optimized, there is–at worst–no change in the performance of the models. That said, the ideal solution might be an ensemble of SMOTE and vanilla models at operating thresholds suited for their flavor.\nThis notebook shows SMOTE impacts models differently, a finding supported by Experimental Perspectives on Learning from Imbalanced Data. They also found, while generally beneficial, SMOTE often did not perform as well as simple random undersampling–something we might try in a future notebook. A different paper, SMOTE for high-dimensional class-imbalanced data, found that for high-dimensional data, SMOTE is beneficial but only after variable selection is performed. The employee attrition problem featured here does not have high-dimensional data, however it is useful to consider how feature selection may impact the calculated Euclidean distance used in the SMOTE algorithm. If we gather more features, it may be beneficial to perform more rigorous feature selection before SMOTE to improve model performance."
  }
]